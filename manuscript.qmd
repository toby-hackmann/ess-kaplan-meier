---
title: "Effective sample size for the Kaplan-Meier estimator: An intuitive measure of uncertainty"
authors:
  - name: Toby Hackmann
    affiliation: 
      - ref: lumc
    roles: analysis, writing
    corresponding: true
  - name: Doranne Thomassen
    affiliation: 
      - ref: lumc
    roles: conceptualization
  - name: Anne M Stiggelbout,
    affiliation: 
      - ref: lumc
  - name: Saskia le Cessie
    affiliation:
      - ref: lumc
      - ref: epi
  - name: Hein Putter
    affiliation: 
      - ref: lumc
  - name: Liesbeth C de Wreede
    affiliation: 
      - ref: lumc
  - name: Ewout W Steyerberg
    affiliation: 
      - ref: lumc
      - ref: julius
    
affiliations:
  - id: lumc
    name: Department of Biomedical Data Sciences, Leiden University Medical Center, Leiden, the Netherlands
  - id: epi
    name: Department of Clinical Epidemiology, Leiden University Medical Center, Leiden, the Netherlands
  - id: julius
    name: Julius Center for Health Sciences and Primary Care, University Medical Center Utrecht, Utrecht, the Netherlands
    
bibliography: references.bib

abstract: | 
    Sample size is an essential indicator of the uncertainty in clinical research results. When studies present time-to-event outcomes with Kaplan-Meier curves, these are often accompanied by the remaining number of patients at risk in a table below the curve. The number at risk at time t informs about uncertainty of the hazard at t, rather than the uncertainty of the estimated survival probability until t,S ̂(t). We aim to review the role of the effective sample size of S ̂(t) to reflect the uncertainty in survival probability estimation. Effective sample size is defined as the size of a hypothetical sample with complete follow-up until time t, that would give the same variance as the variance of the Kaplan-Meier estimate S ̂(t). Illustrations in hypothetical scenarios and in a publicly available dataset support that effective sample size provides a readily interpretable measure of uncertainty for survival curves in the presence of censoring. We show that effective sample size can also quantify the loss of information when reporting for an ongoing study is moved to an earlier time point. Effective sample size is a valuable measure that could be used more often in survival analysis.
   
keywords:
  - sample size
  - kaplan-meier
  - survival
  - risk communication
  
funding: "Funded by the European Union under Horizon Europe Work Programme 101057332. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Health and Digital Executive Agency (HaDEA). Neither the European Union nor the granting authority can be held responsible for them. The UK team are funded under the Innovate UK Horizon Europe Guarantee Programme, UKRI Reference Number: 10041120."
---

```{r}
#| label: initialize
#| output: FALSE
#| eval: FALSE
#| include: FALSE

# To reproduce the results
set.seed(19231030)

# Packages required
pkgs <- c(
  "grid",
  "gridExtra",
  "plotly"
)

# Check if the package is installed, if yes load, if no: install + load 
vapply(pkgs, function(pkg) {
  if (!require(pkg, character.only = TRUE)) install.packages(pkg)
  require(pkg, character.only = TRUE, quietly = TRUE)
}, FUN.VALUE = logical(length = 1L))

rm(pkgs)

source( "code/compile.R" )

# load data
colon_df <- colon
names <- c('sex', 'obstruct', 'perfor', 'differ', 'extent', 'surg', 'node4', 'etype')
colon_df <- colon_df |> 
  mutate( across( all_of(names), as.numeric ) )
rm(names)
colon_os <- colon_df[colon_df$etype == 2, ]
colon_rfs <- colon_df[colon_df$etype == 1, ]

colon_os$time <- colon_os$time/365.25
```

# Introduction
Uncertainty in results from clinical studies is lower with larger sample sizes. Sample size hence is an intuitive measure of uncertainty [@sedlmeier1997]. For survival, the time of the event of interest is typically not observed for all study participants (‘censoring’). Censoring may occur for two main reasons: patients can drop out of the study (‘lost to follow-up’), or they can be administratively censored when the study observation period ends. In both cases, a common assumption in survival analysis is that censoring at time t provides no additional information about prognosis after $t$ [@fojo2021]. In medical research and related fields, the Kaplan-Meier product limit estimator is widely used to provide survival probabilities over time while respecting censoring [@kaplanmeier1958]. Kaplan-Meier curves are often accompanied by the remaining number of patients at risk in a table below the curve. The number at risk at time $t$ informs about uncertainty of the hazard at $t$, and survival after $t$. 

In the current paper we focus on the uncertainty of the estimated survival probability until $t$. We aim to examine the value of ‘effective sample size’ for this uncertainty. Effective sample size is defined as the sample size equivalent to the sample size of a study without censoring [@kaplanmeier1958]. Although, the concept of effective sample size has been discussed with varying definitions and purposes [@cutler1947; @cutler1958; @peto1977; @rothman1978; @dorey1987; @meier2004], it has not often been used as a measure to communicate the uncertainty of estimated survival probabilities. We illustrate effective sample size in a case study and in simulations to better understand its behavior and potential roles in survival analysis. We end with a discussion on utility of the effective sample size and future research.



# Methods

## Definition
Effective sample size has been defined by @kaplanmeier1958 as a sample size, “which in the absence of losses would give the same variance [as the variance of the Kaplan-Meier estimate]”. Suppose we have estimated the survival probability at the time of interest $\hat S(t)$ using the Kaplan-Meier estimate. The variance of $\hat S(t)$ can be consistently estimated by Greenwood's formula [@greenwood1926; @andersen1993]. The effective sample size at time $t$ is defined as the size of a hypothetical sample with complete follow-up (no censoring) until time $t$, with mean survival  $\bar S(t)$ equal to $\hat S(t)$, such that the variance of $\hat S(t)$ is the same as the sampling variance of  $\bar S(t)$ in the hypothetical population. To calculate this effective sample size, we equate the estimated variance of the Kaplan-Meier estimated survival outcome $\hat S(t)$ with the binomial sampling variance of the mean outcome $\bar S(t)$. 

$$
\hat V_{GW}[\hat S(t) ]  = V_{Bin}[ \bar S(t) ],
$$ {#eq-km-greenwood}
which yields
$$
\hat S ^2(t)\sum_{t_i \leq t}\frac{\delta_i}{R_i(R_i-\delta_i)} = \frac{\bar S(t)(1-\bar S(t))}{N}, 
$$ {#eq-equiv}

with $t_1 < \dots <t_D$ the $D$ distinct times where events are observed and $\delta_i$ are the number of events and $R_i$ the number of patients at risk at time $t_i$. The Greenwood estimator for the variance of the Kaplan-Meier estimator is used on the left hand side [@greenwood1926]. We replace the sample mean survival with the estimated survival, and the complete sample size with the effective sample size. Now @eq-equiv can be rewritten to provide the estimator for effective sample size:
$$
\hat N_{\text{eff}}(t) = \frac{1-\hat S(t)}{\hat S (t)\sum_{i: t_i \leq t}\frac{\delta_i}{R_i(R_i-\delta_i)}}.
$$ {#eq-effective-n}

The estimator for effective sample size is undefined before the first event since $\sum_{i: t_i \leq t}\frac{\delta_i}{R_i(R_i-\delta_i)} = 0$. Before the first event, we define effective sample size as the number of patients at risk of an event.
This quantity is the ‘effective sample size for estimates based on the Kaplan-Meier estimator’, but for brevity and readability we refer to $N_{\text{eff}}$ for effective sample size.


## Modified effective sample size
The Kaplan-Meier survival estimate with Greenwood variance and resulting confidence intervals only change at times with events. After the last event the standard $N_{\text{eff}}$ estimator (@eq-effective-n) is constant at time points where no events occur but patients are censored. We consider this a limitation of the above definition of $N_{\text{eff}}$, because, intuitively, censoring should decrease the effective sample size. Methods have been proposed to improve the variance and confidence interval estimation in the tail [@peto1977].  One such method is to modify the variance estimation procedure when there is censoring but no event [@dorey1987]. This method provides better coverage but is also more conservative than the standard Greenwood estimator [@yuan2011].

Using the modified variance suggested by @dorey1987, instead of Greenwood’s variance, we derive a modified effective sample size $N_{\text{eff, mod}}$ (mathematical derivation in Supplement 1). $N_{\text{eff, mod}}$ decreases at times $t$ with only censoring and therefore decreases after the last observed event. We refer to both effective sample size and modified effective sample size with $N_{\text{eff, (mod)}}$.


## Illustrative clinical data
We illustrate the behavior of effective sample size using a publicly available colon cancer data set with the `survival` package in `R` [@package-survival]. The data set includes patients from a clinical trial of adjuvant chemotherapy after a resection with curative intent for a histologically confirmed adenocarcinoma of the colon or rectum [@laurie1989; @moertel1995]. Patients were randomized to either observation (Obs), treatment with levimasole (Lev) alone, or treatment with levimasole and fluorouracil (Lev+5FU). Overall survival and recurrence-free survival were end points. We focused our analyses on overall survival.


## Simulation study: Dependence on hazard functions
We examined the behavior of $N_{\text{eff, (mod)}}$ further in some simulated settings. In the first simulation we varied hazards for events and censoring in the `simsurv` package [@package-simsurv]. We generated survival times $T_E$ and censoring times $T_C$ from Weibull hazard functions. An event was observed at time $T_E$ if $T_E\leq T_C$ and a patient was censored at time $T_C$ if $T_C<T_E$. All patients were administratively censored at $t=5$.  We generated survival data using Weibull$(a , b)$ distributions, with $a$ representing the shape parameter, and $b$ the scale parameter, in such a way that the cumulative hazard was

$$
H(t) = \left(\frac{t}{b}\right)^a.
$$ {#eq-rweibull}

Three different Weibull distributions were used for the event and censoring time generation, yielding a total of 9 combinations. Shape parameters $(a)$ were chosen such that events/censoring occurred early $(a=4)$, evenly spread $(a=1)$, or late $(a=\frac{1}{4})$ during follow-up. The scale parameters $(b)$, defining when the cumulative hazard reaches $1$, was set to $b=2.5$ for event simulations, while $b=3.5$ was used for censoring (Figure S1 in Supplement 2). This difference ensures that, on average, more patients will have an event than be censored with the same shape parameter. Each simulated sample consisted of 1,000 patients. The estimated survival function, $N_{\text{eff, (mod)}}$, the number of patients uncensored and the number at risk were averaged over 50,000 replications for stability. The number at risk is defined as the number of patients who were both alive and still in follow-up. The number not censored is defined as the sum of the number at risk and the number of previous events. 


```{r}
#| eval: FALSE
#| output: FALSE
#| include: FALSE
#| fig-width: 7
#| fig-asp: 0.618
#| fig-cap: |
#|   Cumulative hazards of the different simulations. Early, constant and late simulations are goverened by the shape parameters set at $\frac{1}{4},\ 1$ or $4$. To generate a larger proportion of events compared to censorings, the scale parameter for event simulation is 2.5 and for censoring time simulation it is 3.5. The cumulative hazard reaches 1 at these times.
#| warning: FALSE

# Generate Weibull distributed data
data1 <- -log( 1- pweibull(seq(0, 5, by = 0.01), shape = 1/4, scale = 2.5) )
data2 <- -log( 1- pweibull(seq(0, 5, by = 0.01), shape = 1, scale = 2.5) )
data3 <- -log( 1- pweibull(seq(0, 5, by = 0.01), shape = 4, scale = 2.5) )
data4 <- -log( 1- pweibull(seq(0, 5, by = 0.01), shape = 1/4, scale = 3.5) )
data5 <- -log( 1- pweibull(seq(0, 5, by = 0.01), shape = 1, scale = 3.5) )
data6 <- -log( 1- pweibull(seq(0, 5, by = 0.01), shape = 4, scale = 3.5) )
#data7 <- -log( 1- pweibull(seq(0, 5, by = 0.01), shape = 0.5, scale = 3) )

# Combine density estimates into a data frame
df <- data.frame(
  x = c(seq(0, 5, by = 0.01), seq(0, 5, by = 0.01), seq(0, 5, by = 0.01), seq(0, 5, by = 0.01), seq(0, 5, by = 0.01), seq(0, 5, by = 0.01)#, seq(0, 5, by = 0.01)
        ),
  y = c(data1, data2, data3, data4, data5, data6#, data7
        ),
  group = factor(rep(c("Early Event", "Constant Event", "Late Event", "Early Censor", "Constant Censor", "Late Censor"#, "Trial Simulation"
                       ), 
                     each = length(data1)))
)

# Define custom colors
custom_colors <- c("Early Event" = "#2E7691", 
                   "Constant Event" = "#C2666B", 
                   "Late Event" = "#c6aa2c",
                   "Early Censor" = "#86c2d9",
                   "Constant Censor" = "#e0b2b5",
                   "Late Censor" = "#e2cf7b"#,
                   #"Trial Simulation" = "#9f84af"
                   )

# Plot using ggplot2
plot <- ggplot(df, aes(x = x, y = y, color = group)) +
  geom_line( linewidth = 1.2 ) +
  scale_color_manual(values = custom_colors) +
  xlim(0, 5) +
  ylim(0, 3) +
  labs(title = "Comparison of the cumulative hazards for simulation",
       x = "Time (years)",
       y = "Cumulative hazard",
       color = "Simulation") +
  theme_minimal()
plot
#ggsave(file = "images/cumhaz.svg", plot=plot, width = 10, height = 7)

rm(data1, data2, data3, data3, data4, data5, data6, #data7, 
   df, custom_colors)
```



## Quantifying loss of information for early study reporting
We investigated the potential role of effective sample size in informing about the loss of information in reporting the results of a cohort study or trial early, with illustration in the colon data. The outcome of interest was the 5-year overall survival in the three arms of the trial. For this illustration, we generated enrollment times and event times on the scale of calendar time of the study (Supplement 3).

All analyses were conducted in `R` [@R] version 4.3.1 and specifically the `survival` package [@package-survival]. The functions used to estimate the effective sample size work on the `survfit` object of (stratified) Kaplan-Meier models and can be found on [Github](https://github.com/toby-hackmann/ess-kaplan-meier).



# Results

## Illustration in colon cancer data
Overall survival was $46\%$ [95% condifdence interval 41--51%] at $8$ years in the colon cancer data set with 452 observed events among 929 patients (@fig-colon A). Before any patient was censored, $N_{\text{eff, (mod)}}$ was 929, equal to the total sample size. After patients were censored, $N_{\text{eff, (mod)}}$ began to decrease. The number at risk was much smaller than $N_{\text{eff, (mod)}}$ at later times $t$ (@fig-colon B). The number not censored decreased earlier than $N_{\text{eff, (mod)}}$, because of events occuring early rather than censoring.

```{r}
#| eval: FALSE
#| include: FALSE
#| output: FALSE
#| label: fig-n-eff-km
#| fig-height: 5
#| fig-asp: 0.618
#| fig-cap: |
#|   Kaplan-Meier estimate of the complete data from the illustrative colon dataset. Next to it is the effective sample size and modified effective sample size of the estimates. For comparison the number of patients at risk is included, as well as the number of patients who have not been censored, which could be considered a 'complete case analysis' sample size.
#| fig-subcap:
#|  - "Plot of the Kaplan-Meier estimate of the overall survival in the colon data set."
#|  - "Effective sample size of the Kaplan-Meier estimate of the overall survival in the colon data set."
#| layout-ncol: 2


obj <- survfit( Surv( time, status ) ~ 1, data = colon_os ) |>
  survfit_n()

# Plot the KM
p1 <- plot_km_eff2( obj, both = F, title = "Kaplan-Meier curve", xlab = "Time (years)")
#ggsave(file = "images/colon_km.svg", plot=p1, width = 9, height = 10)
p1

# Plot the effective N
p2 <- plot_effective_n( obj, mod = T, survival = F, bounds = F, title = "Effective sample size", xlab = "Time (years)", xmax = 8 )
#ggsave(file = "images/colon_effn.svg", plot=p2, width = 12, height = 10)
p2
```

![Kaplan-Meier estimate of overall survival based on the complete data from the colon dataset (A), with corresponding indicators of sample size (B). Effective sample size (standard or modified) is always higher than the number at risk and only begins to decrease after patients are censored. Almost no censoring occurred before 5 years of follow-up $(t=5)$.](images/colon_ill_3.jpg){#fig-colon}

The Kaplan-Meier estimated survival curve was constant after \~8 years (@fig-colon A), yielding a constant estimate of $N_{\text{eff}}$ in the tail while -by definition- $N_{\text{eff, mod}}$ decreased with continued censoring of patients. At 8 years, $N_{\text{eff}}$ for the overall survival estimate was 385 and $N_{\text{eff, mod}}$ was 349, while the number at risk was only 26. 


![Kaplan-Meier estimate of overall survival based on the complete data from the colon dataset (A), with corresponding indicators of sample size (B). Effective sample size (standard or modified) is always higher than the number at risk and only begins to decrease after patients are censored. Almost no censoring occurred before 5 years of follow-up $(t=5)$.](images/km_tx.jpg){#fig-tx}
```{r}
#| output: FALSE
#| eval: FALSE
#| warning: FALSE
#| label: fig-tx-auto
#| fig-width: 10
#| fig-asp: 0.7
#| fig-cap: |
#|   Kaplan-Meier curve of the colon data stratified by treatment arm. Effective sample size provides a more optimistic value of the amount of information that the estimator is based on, when compared to the number at risk. While there seems to be no real difference in censoring patterns, $N_{\text{eff}}$ of the three curves at $t=8$ are quite different due to the different times at which the last event occured in the three samples. Modified effective sample size is very similar for the three curves at $t=8$. 
survfit( Surv( time, status) ~ rx, data = colon_os ) |>
  survfit_n( ) |> 
  plot_km_eff( both = T, mark = T, title = "Kaplan-Meier with effective sample size", xlab = "Time (years)", legend.pos = c(0.15, 0.5) ) 
```

We note that $N_{\text{eff}}$ at the tail of the curve was highly dependent on the timing of the last event in the three treatment groups ($t=8$ @fig-colon B, @fig-tx). The cohort with Lev+5FU has the earliest last event and the highest $N_{\text{eff}}$ (210) at $t=8$, and the cohort with only Lev has the latest last event and the lowest $N_{\text{eff}}$ (75). $N_{\text{eff, mod}}$ avoided this dependency, with more similar effective sample sizes at $t=8$ (75 to 103) than $N_{\text{eff}}$ (75 to 210, @fig-tx).


## Dependence of $N_{\text{eff}}$ on hazards
In the simulation scenario of early censoring (left column @fig-simulation) $N_{\text{eff, (mod)}}$ decreased more rapidly. As time continued, $N_{\text{eff, (mod)}}$ remained close to the number of uncensored patients. Patients censored early during follow-up had a large impact on $N_{\text{eff, (mod)}}$, which decreased by approximately 1 per censored patient, while the impact of censoring on $N_{\text{eff, (mod)}}$ in the tail was lower. In the simulation scenario with a constant hazard of censoring an approximately linear decrease in effective sample size over time occurred (middle column @fig-simulation). The late censoring scenario (right column @fig-simulation), resembled the colon data with most events occurring early in the follow-up. Initially, $N_{\text{eff, (mod)}}$ was close to the complete sample size. At later times $t$, $N_{\text{eff, (mod)}}$ decreased drastically, to numbers far below the number of uncensored patients. Late censoring combined with early events showed the largest difference between $N_{\text{eff}}$  and $N_{\text{eff, mod}}$. 


![Results of the simulations based on the hazards proposed in Figure 1. The simulations are based on 1000 patients per simulation and are the average of 50,000? repetitions. Early, constant and late censoring results are shown in the left, middle and right columns. Event hazards are different between the top, middle and bottom rows. The shape of $N_{\text{eff}}$ over time depends mostly on the censoring hazard.](images/sim9.jpg){#fig-simulation}

```{r}
#| eval: FALSE
#| include: FALSE
#| label: fig-simulation
#| fig-cap: Results of the simulations based on the hazards proposed in @fig-cumhaz. The simulations are based on 1000 patients per simulation and are the average of 50 repetitions to smoothen the results and avoid extreme outliers. Early, constant and late censoring results can be seen in the left, middle and right columns. Event hazards are different between the top, middle and bottom rows. We can observe that the shape of effective sample size over time depends mostly on the censoring hazard.
#| fig-width: 14
#| fig-asp: 1

# Seed for consistency
set.seed(111923)


scale = c(2.5, 3.5) # scale[1] for events, scale[2] for censoring
shape = c(1/4, 1, 4) # shape[1] for early, [2] for middle and [3] for late
iter = 10 # number of iterations
n = 1000 # number of patients
maxtime = 5 # maxtime
time = seq(0, maxtime, 0.002) # timegrid for plot


ee <- ec <- el <- ce <- cc <- cl <- le <- lc <- ll <- 0
# Multiple simulations - average 1 at a time
for ( i in 1:iter ){
  # Generate the six datasets of 1000 patients
  e_early <- simsurv( dist="weibull", 
                         gammas=shape[1], 
                         lambdas=scale[1]^(-shape[1]), 
                         x=data.frame(id = 1:n),
                         maxt=maxtime)
  e_middle <- simsurv( dist="weibull", 
                         gammas=shape[2], 
                         lambdas=scale[1]^(-shape[2]), 
                         x=data.frame(id = 1:n),
                         maxt=maxtime)
  e_late <- simsurv( dist="weibull", 
                         gammas=shape[3], 
                         lambdas=scale[1]^(-shape[3]), 
                         x=data.frame(id = 1:n),
                         maxt=maxtime)
  c_early <- simsurv( dist="weibull", 
                         gammas=shape[1], 
                         lambdas=scale[2]^(-shape[1]), 
                         x=data.frame(id = 1:n),
                         maxt=maxtime)
  c_middle <- simsurv( dist="weibull", 
                         gammas=shape[2], 
                         lambdas=scale[2]^(-shape[2]), 
                         x=data.frame(id = 1:n),
                         maxt=maxtime)
  c_late <- simsurv( dist="weibull", 
                         gammas=shape[3], 
                         lambdas=scale[2]^(-shape[3]), 
                         x=data.frame(id = 1:n),
                         maxt=maxtime)
  
  # Build the 9 dataframes for the figures
  early_early <- data.frame(
    time = pmin(e_early$eventtime, c_early$eventtime),
    status = ifelse(e_early$eventtime == maxtime & c_early$eventtime == maxtime, 0,
                    ifelse(e_early$eventtime <= c_early$eventtime, 1, 0))
  )
  early_middle <- data.frame(
    time = pmin(e_early$eventtime, c_middle$eventtime),
    status = ifelse(e_early$eventtime == maxtime & c_middle$eventtime == maxtime, 0,
                    ifelse(e_early$eventtime <= c_middle$eventtime, 1, 0))
  )
  early_late <- data.frame(
    time = pmin(e_early$eventtime, c_late$eventtime),
    status = ifelse(e_early$eventtime == maxtime & c_late$eventtime == maxtime, 0,
                    ifelse(e_early$eventtime <= c_late$eventtime, 1, 0))
  )
  middle_early <- data.frame(
    time = pmin(e_middle$eventtime, c_early$eventtime),
    status = ifelse(e_middle$eventtime == maxtime & c_early$eventtime == maxtime, 0,
                    ifelse(e_middle$eventtime <= c_early$eventtime, 1, 0))
  )
  middle_middle <- data.frame(
    time = pmin(e_middle$eventtime, c_middle$eventtime),
    status = ifelse(e_middle$eventtime == maxtime & c_middle$eventtime == maxtime, 0,
                    ifelse(e_middle$eventtime <= c_middle$eventtime, 1, 0))
  )
  middle_late <- data.frame(
    time = pmin(e_middle$eventtime, c_late$eventtime),
    status = ifelse(e_middle$eventtime == maxtime & c_late$eventtime == maxtime, 0,
                    ifelse(e_middle$eventtime <= c_late$eventtime, 1, 0))
  )
  late_early <- data.frame(
    time = pmin(e_late$eventtime, c_early$eventtime),
    status = ifelse(e_late$eventtime == maxtime & c_early$eventtime == maxtime, 0,
                    ifelse(e_late$eventtime <= c_early$eventtime, 1, 0))
  )
  late_middle <- data.frame(
    time = pmin(e_late$eventtime, c_middle$eventtime),
    status = ifelse(e_late$eventtime == maxtime & c_middle$eventtime == maxtime, 0,
                    ifelse(e_late$eventtime <= c_middle$eventtime, 1, 0))
  )
  late_late <- data.frame(
    time = pmin(e_late$eventtime, c_late$eventtime),
    status = ifelse(e_late$eventtime == maxtime & c_late$eventtime == maxtime, 0,
                    ifelse(e_late$eventtime <= c_late$eventtime, 1, 0))
  )
  ee <- ee + 1/iter*(survfit( Surv( time, status ) ~ 1, data = early_early ) |>
                       survfit_n() |>
                       sf_to_df( time = time ))
  ec <- ec + 1/iter*(survfit( Surv( time, status ) ~ 1, data = early_middle ) |>
                       survfit_n() |>
                       sf_to_df( time = time ))
  el <- el + 1/iter*(survfit( Surv( time, status ) ~ 1, data = early_late ) |>
                       survfit_n() |>
                       sf_to_df( time = time ))
  ce <- ce + 1/iter*(survfit( Surv( time, status ) ~ 1, data = middle_early ) |>
                       survfit_n() |>
                       sf_to_df( time = time ))
  cc <- cc + 1/iter*(survfit( Surv( time, status ) ~ 1, data = middle_middle ) |>
                       survfit_n() |>
                       sf_to_df( time = time ))
  cl <- cl + 1/iter*(survfit( Surv( time, status ) ~ 1, data = middle_late ) |>
                       survfit_n() |>
                       sf_to_df( time = time ))
  le <- le + 1/iter*(survfit( Surv( time, status ) ~ 1, data = late_early ) |>
                       survfit_n() |>
                       sf_to_df( time = time ))
  lc <- lc + 1/iter*(survfit( Surv( time, status ) ~ 1, data = late_middle ) |>
                       survfit_n() |>
                       sf_to_df( time = time ))
  ll <- ll + 1/iter*(survfit( Surv( time, status ) ~ 1, data = late_late ) |>
                       survfit_n() |>
                       sf_to_df( time = time ))
}




# Early-early
plot1 <- plot_effective_n( ee, mod = T, survival = F, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_x = F, ticks_y_sec = F, ylim = c(0, n) )

# Early-middle
plot2 <- plot_effective_n( ec, mod = T, survival = F, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_x = F, ticks_y = F, ticks_y_sec = F, ylim = c(0, n) )

# Early-late
plot3 <- plot_effective_n( el, mod = T, survival = F, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_x = F, ticks_y = F, ylim = c(0, n) )

# Middle-early
plot4 <- plot_effective_n( ce, mod = T, survival = F, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_x = F, ticks_y_sec = F, ylim = c(0, 1000) ) #+
  #annotate("text", x = 0, y = 480, label = "Patients", angle = 90)

# Middle-middle
plot5 <- plot_effective_n( cc, mod = T, survival = F, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_x = F, ticks_y = F, ticks_y_sec = F, ylim = c(0, n) )

# Middle-late
plot6 <- plot_effective_n( cl, mod = T, survival = F, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_x = F, ticks_y = F, ylim = c(0, n) ) #+
  #annotate("text", x = 5, y = 500, label = "Incidence", angle = 270)

# Late-early
plot7 <- plot_effective_n( le, mod = T, survival = F, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_y_sec = F, ylim = c(0, n) )

# Late-middle
plot8 <- plot_effective_n( lc, mod = T, survival = F, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_y = F, ticks_y_sec = F, ylim = c(0, n) ) #+
  #annotate("text", x = 2.5, y = 20, label = "Time")

# Late-late
plot9 <- plot_effective_n( ll, mod = T, survival = F, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_y = F, ylim = c(0, n) )

# Combine plots into list
plots <- list( plot1, plot2, plot3, plot4, plot5, plot6, plot7, plot8, plot9 )

# Column and row labels
col_labels <- c("Early censoring", "Constant censoring", "Late censoring")
row_labels <- c("Early events", "Constant events", "Late events")

# Create text grobs for column and row labels
col_text <- lapply(col_labels, textGrob, gp = gpar(fontsize = 14))
row_text <- lapply(row_labels, textGrob, gp = gpar(fontsize = 14), rot = 90)

# Create a sample plot to extract the legend
legend_plot <- ggplot(data.frame(x = 1, y = 1, 
                                 color = c("Effective", "Modified", "Uncensored", "At risk", "Incidence"), 
                                 linetype = c("Effective", "Modified", "Uncensored", "At risk", "Incidence")), 
                      aes(x = x, y = y, color = color, linetype = linetype)) +
  geom_line(linewidth = 1.3) +
  scale_color_manual(values = c("Effective"= "#37293F", "Modified" = "#9f84af", "Uncensored" = "#c6aa2c", "At risk" = "#C2666B"), name = "Legend") +
  scale_linetype_manual(values = c("Effective"= "solid", "Modified" = "solid", "Uncensored" = "dotdash", "At risk" = "dashed"), name = "Legend") +
  theme(legend.position = "right") + theme_minimal()
# "Bounds", 
# "Bounds" = "#c6b5cf", 
# "Bounds" = "dashed", 


# Extract the legend
legend <- cowplot::get_legend(legend_plot)

# Create top row with column labels
top_row <- arrangeGrob(
  grobs = c(list(textGrob("")), col_text),
  ncol = 4,
  widths = c(0.1, 732/675, 1, 732/675)
)

# Create each row with plots and row label
rows <- lapply(1:3, function(i) {
  arrangeGrob(
    grobs = c(list(row_text[[i]]), plots[((i-1)*3+1):(i*3)]),
    ncol = 4,
    widths = c(0.1, 732/675, 1, 732/675)
  )
})

# Combine the top row and all plot rows into the final grid
final_grid <- arrangeGrob(
  grobs = c(list(top_row), rows),
  ncol = 1,
  heights = c(0.1, 1, 1, 827/800)
)

# Add legend to the grid
combined_grid <- arrangeGrob(
  final_grid,
  legend,
  ncol = 2,
  widths= c(8, 1)
)


grid.arrange(combined_grid)


#ggsave(file = "images/sim2.svg", plot=combined_grid, width = 12, height = 12, dpi = 700)
#rm( shape, scale, e_early, e_middle, e_late, c_early, c_middle, c_late, 
#    early_early, early_middle, early_late, middle_early, middle_middle, 
#    middle_late, late_early, late_middle, late_late, plot1, plot2, plot3, plot4,
#    plot5, plot6, plot7, plot8, plot9, plots, col_labels, row_labels, col_text, 
#    row_text, legend_plot, legend, top_row, rows, final_grid, combined_grid)
```

```{r}
#| eval: FALSE
#| include: FALSE

# This is just to generate seperate KMs

 e_early <- simsurv( dist="weibull", 
                         gammas=shape[1], 
                         lambdas=scale[1]^(-shape[1]), 
                         x=data.frame(id = 1:100000),
                         maxt=maxtime)
  p <- survfit( Surv( eventtime, status)~1, data = e_early)|>
        survfit_n() |>
        plot_km_eff2( xlab = "Time (years)")
  ggsave(file = "images/sim_km_e.svg", plot=p, width = 7, height = 5)
  
  e_middle <- simsurv( dist="weibull", 
                         gammas=shape[2], 
                         lambdas=scale[1]^(-shape[2]), 
                         x=data.frame(id = 1:100000),
                         maxt=maxtime)
    p <- survfit( Surv( eventtime, status)~1, data = e_middle)|>
        survfit_n() |>
        plot_km_eff2( xlab = "Time (years)")
  ggsave(file = "images/sim_km_m.svg", plot=p, width = 7, height = 5)
  
  e_late <- simsurv( dist="weibull", 
                         gammas=shape[3], 
                         lambdas=scale[1]^(-shape[3]), 
                         x=data.frame(id = 1:100000),
                         maxt=maxtime)
    p <- survfit( Surv( eventtime, status)~1, data = e_late)|>
        survfit_n() |>
        plot_km_eff2( xlab = "Time (years)")
  ggsave(file = "images/sim_km_l.svg", plot=p, width = 7, height = 5)
  
  c_early <- simsurv( dist="weibull", 
                         gammas=shape[1], 
                         lambdas=scale[2]^(-shape[1]), 
                         x=data.frame(id = 1:100000),
                         maxt=maxtime)
    p <- survfit( Surv( eventtime, status)~1, data = c_early)|>
        survfit_n() |>
        plot_km_eff2( xlab = "Time (years)")
  ggsave(file = "images/simc_km_e.svg", plot=p, width = 7, height = 5)
  
  c_middle <- simsurv( dist="weibull", 
                         gammas=shape[2], 
                         lambdas=scale[2]^(-shape[2]), 
                         x=data.frame(id = 1:100000),
                         maxt=maxtime)
    p <- survfit( Surv( eventtime, status)~1, data = c_middle)|>
        survfit_n() |>
        plot_km_eff2( xlab = "Time (years)")
  ggsave(file = "images/simc_km_m.svg", plot=p, width = 7, height = 5)
  
  c_late <- simsurv( dist="weibull", 
                         gammas=shape[3], 
                         lambdas=scale[2]^(-shape[3]), 
                         x=data.frame(id = 1:100000),
                         maxt=maxtime)
    p <- survfit( Surv( eventtime, status)~1, data = c_late)|>
        survfit_n() |>
        plot_km_eff2( xlab = "Time (years)")
  #ggsave(file = "images/simc_km_l.svg", plot=p, width = 7, height = 5)
```

```{r}
#| eval: FALSE
#| include: FALSE
#| label: fig-sim-same-risk-diff-ess
#| fig-cap: Simulation of two same Kaplan-Meier curves, where the censoring distribution is chosen such that the number at risk will be the same for both simulations at some point, while the effective sample size is different.
#| fig-width: 14
#| fig-asp: 1

# Seed for consistency
set.seed(111923)

# scale[1] for events, scale[2] for censoring 1 and scale[3] for censoring 2
scale = c(2.5, 2.5, 2.5)
# shape[1] for events, [2] for censoring 1 and [3] for censoring 2
shape = c(1, 1/2, 2)
# number of iterations
iter = 30
# maxtime
maxtime = 5
# timegrid for plot
time = seq(0, maxtime, 0.002)

# So my setup is as follows. For the survival curve, I take the same for both, a Weibull( 1, 2.5 ), which is also an exponential distribution.
# For the censoring, if I also use scale 2.5 for both of those, then the cumulative hazard for both the events and censoring will be 1 in both
# simulations, which means that the number a risk must be the same as well (asymptotically, not every iteration of course). Then I can 
# change the effective sample size by varying the shape of the censoring distribution. If I change it too harshly, then the survival
# function may be affected, so I choose 2 and 1/2 for it.

dat1 <- dat2 <- 0
# Multiple simulations - average 1 at a time
for ( i in 1:iter ){
  # Generate the six datasets of 1000 patients
  events <- simsurv( dist="weibull", 
                         gammas=shape[1], 
                         lambdas=scale[1]^(-shape[1]), 
                         x=data.frame(id = 1:1000),
                         maxt=maxtime)
  censoring_1 <- simsurv( dist="weibull", 
                         gammas=shape[2], 
                         lambdas=scale[2]^(-shape[2]), 
                         x=data.frame(id = 1:1000),
                         maxt=maxtime)
  censoring_2 <- simsurv( dist="weibull", 
                         gammas=shape[3], 
                         lambdas=scale[3]^(-shape[3]), 
                         x=data.frame(id = 1:1000),
                         maxt=maxtime)
  
  # Build the 2 dataframes for the figures
  df_1 <- data.frame(
    time = pmin(events$eventtime, censoring_1$eventtime),
    status = ifelse(events$eventtime == maxtime & censoring_1$eventtime == maxtime, 0,
                    ifelse(events$eventtime <= censoring_1$eventtime, 1, 0))
  )
  df_2 <- data.frame(
    time = pmin(events$eventtime, censoring_2$eventtime),
    status = ifelse(events$eventtime == maxtime & censoring_2$eventtime == maxtime, 0,
                    ifelse(events$eventtime <= censoring_2$eventtime, 1, 0))
  )
  dat1 <- dat1 + 1/iter*(survfit( Surv( time, status ) ~ 1, data = df_1 ) |>
                       survfit_n() |>
                       sf_to_df( time = time ))
  dat2 <- dat2 + 1/iter*(survfit( Surv( time, status ) ~ 1, data = df_2 ) |>
                       survfit_n() |>
                       sf_to_df( time = time ))
}

n <- 1000
color_values <- c("Early Censoring"= "#2E7691", "Late Censoring" = "#C2666B")
linetype_values <- c("Effective"= "solid", "At Risk" = "dashed")
# calculate the time where dat1$n.risk and dat2$n.risk cross over, that is, the bigger becomes smaller
plot <- ggplot() +
    geom_step(aes(x = dat1$time, y = dat1$n.eff, color = "Early Censoring", linetype = "Effective"), linewidth = 1.3) +
    geom_step(aes(x = dat1$time, y = dat1$n.risk, color = "Early Censoring", linetype = "At Risk"), linewidth = 1.3) +
    geom_step(aes(x = dat2$time, y = dat2$n.eff, color = "Late Censoring", linetype = "Effective"), linewidth = 1.3) +
    geom_step(aes(x = dat2$time, y = dat2$n.risk, color = "Late Censoring", linetype = "At Risk"), linewidth = 1.3) +
    labs(x = "Time", y = "Number") +
    scale_x_continuous(breaks = seq(0, maxtime, length.out = 5)) +
    theme_minimal() +
    theme(legend.position.inside = c(0.1, 0.4), panel.grid.major.x = element_blank()) +
    theme(text = element_text(size = 12), plot.title = element_text( size = 18, hjust = 0.3)) +
    scale_color_manual(values = color_values, name = "Legend") +
    scale_linetype_manual(values = linetype_values, name = "Legend") + 
    geom_vline(xintercept = 2.5, linetype = "dotted", lwd = 1.3)
plot
ggsave(file = "images/hein_sim_n.svg", plot=plot, width = 7, height = 5)

plot2 <- ggplot() +
    geom_step(aes(x = dat1$time, y = dat1$surv, color = "Early Censoring"), linewidth = 1.3) +
    geom_step(aes(x = dat2$time, y = dat2$surv, color = "Late Censoring"), linewidth = 1.3) +
    labs(x = "Time", y = "Survival probability") +
    scale_x_continuous(breaks = seq(0, maxtime, length.out = 5)) +
    theme_minimal() +
    theme(legend.position.inside = c(0.1, 0.4), panel.grid.major.x = element_blank()) +
    theme(text = element_text(size = 12), plot.title = element_text( size = 18, hjust = 0.3)) +
    scale_color_manual(values = color_values, name = "Legend") +
    scale_linetype_manual(values = linetype_values, name = "Legend") + 
    geom_vline(xintercept = 2.5, linetype = "dotted", lwd = 1.3)
plot2
#ggsave(file = "images/hein_sim_surv.svg", plot=plot2, width = 7, height = 5)
```

Comparisons between the rows in @fig-simulation illustrate how censoring hazards cause different patterns of $N_{\text{eff, (mod)}}$ for the survival estimate. There are also differences in the numbers at risk between these scenarios at most times $t$. However, at a specific time point $t_*$, the number at risk may be exactly the same while there still are clear differences in $N_{\text{eff, (mod)}}$ (Figure S3 in Supplement 4).

## Effective sample size to guide study reporting
Effective sample size can inform how much information is available for estimating the survival probability at the time point $t$ of interest. Maximum information is available when all patients have either experienced an event or reached the time point of interest event-free as uncensored observations.
For the colon data, we focus on the 5-year survival estimate $t=5$. Five years after start of the study, the first enrolled patient may have sufficient follow-up for a $t=5$ survival estimate. When accrual lasts 4 years, the last enrolled patient has 5 years of follow-up at year 9 after start of the study. We note that 95\% of the effective sample size is already achieved at year 8 after start of the study, that is one year before the last patient reaches $t=5$ (@fig-trial). This pattern of available information was confirmed for other trial simulations (Figure S5 in Supplement 5).


```{r}
#| eval: FALSE
#| output: FALSE
#| include: FALSE
#| label: fig-sim-trial
#| fig-width: 10
#| fig-asp: 0.5
#| fig-cap: |
#|   Results of a simulated trial for two-year survival after a two-year enrollment period. The three graphs illustrate the trajectory of effective sample size for reporting the results at different times after study initiation. After two years, only the first enrolled patient will have the follow-up of two year that is of interest. After three years, half the patients will have complete follow-up. Four years after the study started, even the last enrolled patient will have the complete two years of follow-up and effective sample size will always be equal to complete sample size.

# Seed for consistency
set.seed(111923)

# Scale and Shape
scale = 3
shape = 0.5

# number of iterations
iter = 1
# timegrid for plot
maxtime = 2
time = seq(0, maxtime, 0.002)


# Multiple simulations - average 1 at a time
for ( i in 1:iter ){
  # Generate the survival time
  survival <- simsurv( dist="weibull", 
                         gammas=shape, 
                         lambdas=scale^(-shape), 
                         x=data.frame(id = 1:250),
                         maxt=maxtime)
  
  # Generate enrollment time
  survival$tstart <- runif(250, 0, 2)
  
  # Survival in 'real' time since start study
  survival$tstop <- survival$tstart + survival$eventtime
  
  
  four <- data.frame(
  time = survival$eventtime,
  status = survival$status
  )
  
  three <- data.frame(
  time = ifelse( survival$tstop < 3, survival$eventtime, survival$eventtime - (survival$tstop - 3)),
  status = ifelse( survival$tstop < 3, survival$status, 0)
  )

  two <- data.frame(
  time = ifelse( survival$tstop < 2, survival$eventtime, survival$eventtime - (survival$tstop - 2)),
  status = ifelse( survival$tstop < 2, survival$status, 0)
  )
}

# Two years
plot1 <- survfit( Surv(time, status) ~1, data = two ) |> 
  survfit_n() |>
  plot_effective_n( mod = T, survival = T, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_x = T, ticks_y_sec = F )

# Two half years
plot2 <- survfit( Surv(time, status) ~1, data = three ) |> 
  survfit_n() |>
  plot_effective_n( mod = T, survival = T, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_x = T, ticks_y_sec = F )

# Three years
plot3 <- survfit( Surv(time, status) ~1, data = four ) |> 
  survfit_n() |>
  plot_effective_n( mod = T, survival = T, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_x = T, ticks_y_sec = F )

# Combine plots into list
plots <- list( plot1, plot2, plot3 )

# Column and row labels
col_labels <- c("Two years", "Three years", "Four years")

# Create text grobs for column and row labels
col_text <- lapply(col_labels, textGrob, gp = gpar(fontsize = 12))

# Create a sample plot to extract the legend
legend_plot <- ggplot(data.frame(x = 1, y = 1, 
                                 color = c("Effective", "Modified", "Uncensored", "At risk", "Incidence"), 
                                 linetype = c("Effective", "Modified", "Uncensored", "At risk", "Incidence")), 
                      aes(x = x, y = y, color = color, linetype = linetype)) +
  geom_line(linewidth = 1.3) +
  scale_color_manual(values = c("Effective"= "#37293F", "At risk" = "#C2666B", "Uncensored" = "#2E7691", "Modified" = "#9f84af", "Incidence" = "#c6aa2c"), name = "Legend") +
  scale_linetype_manual(values = c("Effective"= "solid", "At risk" = "longdash", "Uncensored" = "dotdash", "Modified" = "solid", "Incidence" = "solid"), name = "Legend") +
  theme(legend.position = "right") + theme_minimal()
# "Bounds", 
# "Bounds" = "#c6b5cf", 
# "Bounds" = "dashed", 


# Extract the legend
legend <- cowplot::get_legend(legend_plot)

# Create top row with column labels
top_row <- arrangeGrob(
  grobs = col_text,
  ncol = 3,
  widths = c(732/675, 1, 732/675)
)

# Create each row with plots and row label
rows <- arrangeGrob(
    grobs = plots[1:3],
    ncol = 3,
    widths = c(732/675, 1, 732/675)
)

# Combine the top row and all plot rows into the final grid
final_grid <- arrangeGrob(
  grobs = c(list(top_row), list(rows)),
  ncol = 1,
  heights = c(0.1, 1)
)

# Add legend to the grid
combined_grid <- arrangeGrob(
  final_grid,
  legend,
  ncol = 2,
  widths= c(7, 1)
)

# Display the final grid
grid.draw(combined_grid)

```

```{r}
#| eval: FALSE
#| output: FALSE
#| label: fig-sim-reporting
#| fig-width: 7
#| fig-asp: 0.618
#| fig-cap: |
#|   Figure that shows the mean modified effective sample size of the two-year Kaplan-Meier survival estimate as a function of time. The baseline cumulative hazard was based on a Weibull(3, 0.5) distribution, with enrollment according to a uniform distribution over the first two years of the study. A treatment was given to group `Treat 2` with a hazard ratio of 0.7. Confidence bands are based on the observed 95% range (2.5-97.5% interquantile range) based on 1000 simulations.

# Seed for consistency
set.seed(111923)

plot <- trial_report( gammas = c(0.5), 
              lambdas = c(3^(-0.5)), 
              x = data.frame( id = 1:500,
                              treat = c(rep(0, 250), rep(1, 250))),
              betas = c(treat = 0.7),
              t_outcome = 2, 
              t_enroll = 2, 
              iter = 250, 
              n_patient = 250,
              points = 25,
              conf = T,
              conf.int = 95,
              proportion = F,
              mod = F,
              ylab = "Effective Sample Size for 2 year survival",
              xlab = "Time since study start",
              title = "Effective sample size over reporting time"
              )
plot
#ggsave(file = "images/trial_std.svg", plot=plot, width = 7, height = 5)
```

![Illustration of modified effective sample size $N_{\text{eff, mod}}$ of the 5-year overall survival Kaplan-Meier estimate at increasing trial reporting times in the colon data stratified by treatment arm. With accrual over 4 years and 5-year follow-up for the last patient, we note that 95% of the information in the study is already available at year 8, one year before the planned end of the study.](images/colon_trial.jpg){#fig-trial}


# Discussion
Sample size is a challenging concept for time-to-event data, because not all patients may have complete follow-up (censoring). The concept of effective sample size can provide a measure with the same interpretation as sample size in standard statistical analyses of binary outcomes. We found that the pattern of the effective sample size depended on the hazard for censoring and the hazard for events. How rapidly effective sample size decreases depends especially on the hazard for censoring, with early censoring making survival estimates for later time points less reliable. We consider the modified version of effective sample size $N_{\text{eff, mod}}$ attractive for a more natural behavior with Kaplan-Meier estimates with late censoring without late events, because $N_{\text{eff, mod}}$ also decreases when there is only censoring. We discuss below why $N_{\text{eff, (mod)}}$ might be valuable to add to Kaplan-Meier curves, for risk communication and for study reporting decisions.


## Effective Sample Size below plots of Kaplan-Meier curves
Guidelines and tutorials on the presentation of survival curves state that the number of patients at risk should be presented below the Kaplan-Meier curve  [@vickers2020; @morris2019; @jager2008; @pocock2002]. Often this is accompanied by a recommendation that the survival curve is truncated when the number at risk falls below a certain percentage of the number non-censored patients (10-20%) [@pocock2002], or below an absolute number (e.g. n<5, or n<10) [@vickers2020]. Number at risk is a measure of the reliability of the hazard at time $t$, which relates to the uncertainty for future survival probability estimates beyond time $t$. In contrast, $N_{\text{eff, (mod)}}$ indicates the reliability of the estimated survival probability at time $t$, which is essential information for patients, physicians and other decision-makers. The contrast in absolute numbers can be large. For example. in the colon data the number at risk at 8 years of follow-up was only 26, while the $N_{\text{eff}}$ of the estimated survival probability was 385 and $N_{\text{eff, mod}}$ was 349.
Effective sample size hence can be useful in addition to or instead of the number of patients at risk in a table below the curve. One might object that effective sample size provides information that is already included in the variance and reflected in a 95% confidence interval. Effective sample size however may be quite intuitive for communication of uncertainty with patients and other stakeholders. Further empirical studies are needed to assess this hypothesis, extending previous comparative research on presentations of Kaplan-Meier curves [@morris2019]. 


## Risk communication
Open communication about uncertainty is an important step in improving trustworthiness in scientific results [@spiegelhalter2017; @spiegelhalter2017address]. Some claimed that it is futile to try to explain uncertainty in predictions to individual patients [@kattan2011]. Also, domain experts, such as physicians, may be hesitant to communicate uncertainty, because such communication may undermine trust in their work [@fischhoff2012]. This is disputed by others [@vanderbles2020], while indeed people may be less likely to base decisions on numbers with low quality of evidence [@schneider2022].
In a well-cited study, the instruction with a nomogram for prostate cancer risk states that physicians should tell a patient: “if we had 100 men exactly like you, we would expect between $\text{risk}\%-10\%$ and $\text{risk}\%+10\%$ to remain free of their disease at 5 years” [@kattan2002]. Such a presentation may well convey the absolute risk to a patient but the presentation suggests that the prediction is based on 100 patients. This nomogram was based on Cox proportional hazards model. In predictions from logistic regression models, patients with exceptional covariate patterns have low effective sample [@thomassen2024]. Moreover, research into effective sample sizes of predictions from machine learning models warns that increased model flexibility reduces effective sample sizes of individual predictions [@thomassen2025].  Future research is needed to define and examine the effective sample size for survival predictions $\hat S(t, \mathbf{x}_{new})$ based on combinations of covariates.
Shared decision making between physicians and patients is increasingly important in modern medicine [@stiggelbout2015; @4Dpicture]. Communication about uncertainty of the evidence supporting treatment choices is important for the autonomy of patients [@parascandola2002]. Reference to uncertainty was noted in only half of consultations with probabilities, mentioned [@engelhardt2017]. Only 21% of those references to uncertainty were about the imprecision of the estimate. Effective sample size could help with communication of such imprecision, or epistemic uncertainty [@calin-jageman2019].
As an example we show a possible communication tool based on the colon data with effective sample size (Figure 5). For the key trial result, the definition of ‘patients like you’ refers to randomized patients who met the in- and exclusion criteria of the trial. For clinical application, in a real-world setting, important prognostic factors should be considered, and the format for presentation may need further refinement.



![Example of using effective sample size in risk communication for the colon trial. The 8-year survival is estimated as 41% with surgery only and as 56% with the addition of Lev+5FU chemotherapy after surgery. The effective sample size sizes are 153 and 210 patients respectively.](images/communication.jpg){#fig-example}

We expect that uncertainty communication using effective sample size may not be preferred by all patients. Patients with high numeracy and statistical literacy may appreciate the 95% confidence interval for a survival prediction, while other patients may prefer simpler expressions of uncertainty such as verbal descriptions rather than numbers [@medendorp2021; @vromans2021].


## Trial monitoring
Researchers involved in prospective studies with a survival outcome may struggle with the timing of reporting of results. In oncological and other trials, it is common to require a minimum follow-up for the last patient. This causes a gap between inclusion of the last patient and knowledge on the results of a trial, while the results are important to guide further treatment policies. Reporting as early as possible may be pursued, therefore. As illustrated, effective sample size can express the relative information available at earlier versus later time points. For absolute values of sample size, based on a formal power calculation, a higher event rate than anticipated can positively impact the effective sample size. This may allow for earlier analysis and reporting after inclusion of the last patient. In contrast, more censoring by drop out than expected may argue for longer follow-up after the last included patient or increased enrollment for sufficient precision. 
We recognize that changes in study design can inflate type I error rates [@proschan2009]. Further research is required to assess any possible impact on type I error rates from reporting decisions based on effective sample size.



# Conclusion
Effective sample size may be valuable to indicate the uncertainty of survival estimates from the Kaplan-Meier estimator, which has the same interpretation as sample size in studies without censoring. It can be shown below Kaplan-Meier plots and risk tables in addition to or instead of the number at risk, where it may yield an intuitive estimate of reliability for estimated survival probabilities until the time of interest. Future research should explore the role of effective sample size to communicate uncertainty, to guide reporting decisions, and to quantify uncertainty from regression or machine learning models with survival outcomes.
