{
  "hash": "8a6578e523480187c4a15fd94e589f29",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Effective sample size for the Kaplan-Meier estimator: An intuitive measure of uncertainty\"\nauthors:\n  - name: Toby Hackmann\n    affiliation: \n      - ref: lumc\n    roles: analysis, writing\n    corresponding: true\n  - name: Doranne Thomassen\n    affiliation: \n      - ref: lumc\n    roles: conceptualization\n  - name: Anne M Stiggelbout,\n    affiliation: \n      - ref: lumc\n  - name: Saskia le Cessie\n    affiliation:\n      - ref: lumc\n      - ref: epi\n  - name: Hein Putter\n    affiliation: \n      - ref: lumc\n  - name: Liesbeth C de Wreede\n    affiliation: \n      - ref: lumc\n  - name: Ewout W Steyerberg\n    affiliation: \n      - ref: lumc\n      - ref: julius\n    \naffiliations:\n  - id: lumc\n    name: Department of Biomedical Data Sciences, Leiden University Medical Center, Leiden, the Netherlands\n  - id: epi\n    name: Department of Clinical Epidemiology, Leiden University Medical Center, Leiden, the Netherlands\n  - id: julius\n    name: Julius Center for Health Sciences and Primary Care, University Medical Center Utrecht, Utrecht, the Netherlands\n    \nbibliography: references.bib\n\nabstract: | \n    Sample size is an essential indicator of the uncertainty in clinical research results. When studies present time-to-event outcomes with Kaplan-Meier curves, these are often accompanied by the remaining number of patients at risk in a table below the curve. The number at risk at time t informs about uncertainty of the hazard at $t$, rather than the uncertainty of the estimated survival probability until $t$,$\\hat S(t)$. We aim to review the role of the effective sample size of $\\hat S(t)$ to reflect the uncertainty in survival probability estimation. Effective sample size is defined as the size of a hypothetical sample with complete follow-up until time $t$, that would give the same variance as the variance of the Kaplan-Meier estimate $\\hat S(t)$. Illustrations in hypothetical scenarios and in a publicly available dataset support that effective sample size provides a readily interpretable measure of uncertainty for survival curves in the presence of censoring. We show that effective sample size can also quantify the loss of information when reporting for an ongoing study is moved to an earlier time point. Effective sample size is a valuable measure that could be used more often in survival analysis.\n   \nkeywords:\n  - sample size\n  - kaplan-meier\n  - survival\n  - risk communication\n  \nfunding: \"Funded by the European Union under Horizon Europe Work Programme 101057332. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Health and Digital Executive Agency (HaDEA). Neither the European Union nor the granting authority can be held responsible for them. The UK team are funded under the Innovate UK Horizon Europe Guarantee Programme, UKRI Reference Number: 10041120.\"\n---\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# To reproduce the results\nset.seed(19231030)\n\n# Packages required\npkgs <- c(\n  \"grid\",\n  \"gridExtra\",\n  \"plotly\"\n)\n\n# Check if the package is installed, if yes load, if no: install + load \nvapply(pkgs, function(pkg) {\n  if (!require(pkg, character.only = TRUE)) install.packages(pkg)\n  require(pkg, character.only = TRUE, quietly = TRUE)\n}, FUN.VALUE = logical(length = 1L))\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLoading required package: grid\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLoading required package: gridExtra\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLoading required package: plotly\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLoading required package: ggplot2\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'plotly'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nThe following object is masked from 'package:stats':\n\n    filter\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nThe following object is masked from 'package:graphics':\n\n    layout\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout .hidden}\n\n```\n     grid gridExtra    plotly \n     TRUE      TRUE      TRUE \n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nrm(pkgs)\n\nsource( \"code/compile.R\" )\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLoading required package: survival\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLoading required package: tinytex\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nLoading required package: tidyverse\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.4     v readr     2.1.5\nv forcats   1.0.0     v stringr   1.5.1\nv lubridate 1.9.4     v tibble    3.2.1\nv purrr     1.0.4     v tidyr     1.3.1\n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::combine() masks gridExtra::combine()\nx dplyr::filter()  masks plotly::filter(), stats::filter()\nx dplyr::lag()     masks stats::lag()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\nLoading required package: simsurv\n\nLoading required package: matrixStats\n\n\nAttaching package: 'matrixStats'\n\n\nThe following object is masked from 'package:dplyr':\n\n    count\n\n\nLoading required package: ggsurvfit\n\nLoading required package: svglite\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n# load data\ncolon_df <- colon\nnames <- c('sex', 'obstruct', 'perfor', 'differ', 'extent', 'surg', 'node4', 'etype')\ncolon_df <- colon_df |> \n  mutate( across( all_of(names), as.numeric ) )\nrm(names)\ncolon_os <- colon_df[colon_df$etype == 2, ]\ncolon_rfs <- colon_df[colon_df$etype == 1, ]\n\ncolon_os$time <- colon_os$time/365.25\n```\n:::\n\n\n\n\n\n\n# Introduction\nUncertainty in results from clinical studies is lower with larger sample sizes. Sample size hence is an intuitive measure of uncertainty [@sedlmeier1997]. For survival, the time of the event of interest is typically not observed for all study participants (‘censoring’). Censoring may occur for two main reasons: patients can drop out of the study (‘lost to follow-up’), or they can be administratively censored when the study observation period ends. In both cases, a common assumption in survival analysis is that censoring at time t provides no additional information about prognosis after $t$ [@fojo2021]. In medical research and related fields, the Kaplan-Meier product limit estimator is widely used to provide survival probabilities over time while respecting censoring [@kaplanmeier1958]. Kaplan-Meier curves are often accompanied by the remaining number of patients at risk in a table below the curve. The number at risk at time $t$ informs about uncertainty of the hazard at $t$, and survival after $t$. \n\nIn the current paper we focus on the uncertainty of the estimated survival probability until $t$. We aim to examine the value of ‘effective sample size’ for this uncertainty. Effective sample size is defined as the sample size equivalent to the sample size of a study without censoring [@kaplanmeier1958]. Although, the concept of effective sample size has been discussed with varying definitions and purposes [@cutler1947; @cutler1958; @peto1977; @rothman1978; @dorey1987; @meier2004], it has not often been used as a measure to communicate the uncertainty of estimated survival probabilities. We illustrate effective sample size in a case study and in simulations to better understand its behavior and potential roles in survival analysis. We end with a discussion on utility of the effective sample size and future research.\n\n\n\n# Methods\n\n## Definition\nEffective sample size has been defined by @kaplanmeier1958 as a sample size, “which in the absence of losses would give the same variance [as the variance of the Kaplan-Meier estimate]”. Suppose we have estimated the survival probability at the time of interest $\\hat S(t)$ using the Kaplan-Meier estimate. The variance of $\\hat S(t)$ can be consistently estimated by Greenwood's formula [@greenwood1926; @andersen1993]. The effective sample size at time $t$ is defined as the size of a hypothetical sample with complete follow-up (no censoring) until time $t$, with mean survival  $\\bar S(t)$ equal to $\\hat S(t)$, such that the variance of $\\hat S(t)$ is the same as the sampling variance of  $\\bar S(t)$ in the hypothetical population. To calculate this effective sample size, we equate the estimated variance of the Kaplan-Meier estimated survival outcome $\\hat S(t)$ with the binomial sampling variance of the mean outcome $\\bar S(t)$. \n\n$$\n\\hat V_{GW}[\\hat S(t) ]  = V_{Bin}[ \\bar S(t) ],\n$$ {#eq-km-greenwood}\nwhich yields\n$$\n\\hat S ^2(t)\\sum_{t_i \\leq t}\\frac{\\delta_i}{R_i(R_i-\\delta_i)} = \\frac{\\bar S(t)(1-\\bar S(t))}{N}, \n$$ {#eq-equiv}\n\nwith $t_1 < \\dots <t_D$ the $D$ distinct times where events are observed and $\\delta_i$ are the number of events and $R_i$ the number of patients at risk at time $t_i$. The Greenwood estimator for the variance of the Kaplan-Meier estimator is used on the left hand side [@greenwood1926]. We replace the sample mean survival with the estimated survival, and the complete sample size with the effective sample size. Now @eq-equiv can be rewritten to provide the estimator for effective sample size:\n$$\n\\hat N_{\\text{eff}}(t) = \\frac{1-\\hat S(t)}{\\hat S (t)\\sum_{i: t_i \\leq t}\\frac{\\delta_i}{R_i(R_i-\\delta_i)}}.\n$$ {#eq-effective-n}\n\nThe estimator for effective sample size is undefined before the first event since $\\sum_{i: t_i \\leq t}\\frac{\\delta_i}{R_i(R_i-\\delta_i)} = 0$. Before the first event, we define effective sample size as the number of patients at risk of an event.\nThis quantity is the ‘effective sample size for estimates based on the Kaplan-Meier estimator’, but for brevity and readability we refer to $N_{\\text{eff}}$ for effective sample size.\n\n\n## Modified effective sample size\nThe Kaplan-Meier survival estimate with Greenwood variance and resulting confidence intervals only change at times with events. After the last event the standard $N_{\\text{eff}}$ estimator (@eq-effective-n) is constant at time points where no events occur but patients are censored. We consider this a limitation of the above definition of $N_{\\text{eff}}$, because, intuitively, censoring should decrease the effective sample size. Methods have been proposed to improve the variance and confidence interval estimation in the tail [@peto1977].  One such method is to modify the variance estimation procedure when there is censoring but no event [@dorey1987]. This method provides better coverage but is also more conservative than the standard Greenwood estimator [@yuan2011].\n\nUsing the modified variance suggested by @dorey1987, instead of Greenwood’s variance, we derive a modified effective sample size $N_{\\text{eff, mod}}$ (mathematical derivation in Supplement 1). $N_{\\text{eff, mod}}$ decreases at times $t$ with only censoring and therefore decreases after the last observed event. We refer to both effective sample size and modified effective sample size with $N_{\\text{eff, (mod)}}$.\n\n\n## Illustrative clinical data\nWe illustrate the behavior of effective sample size using a publicly available colon cancer data set with the `survival` package in `R` [@package-survival]. The data set includes patients from a clinical trial of adjuvant chemotherapy after a resection with curative intent for a histologically confirmed adenocarcinoma of the colon or rectum [@laurie1989; @moertel1995]. Patients were randomized to either observation (Obs), treatment with levimasole (Lev) alone, or treatment with levimasole and fluorouracil (Lev+5FU). Overall survival and recurrence-free survival were end points. We focused our analyses on overall survival.\n\n\n## Simulation study: Dependence on hazard functions\nWe examined the behavior of $N_{\\text{eff, (mod)}}$ further in some simulated settings. In the first simulation we varied hazards for events and censoring in the `simsurv` package [@package-simsurv]. We generated survival times $T_E$ and censoring times $T_C$ from Weibull hazard functions. An event was observed at time $T_E$ if $T_E\\leq T_C$ and a patient was censored at time $T_C$ if $T_C<T_E$. All patients were administratively censored at $t=5$.  We generated survival data using Weibull$(a , b)$ distributions, with $a$ representing the shape parameter, and $b$ the scale parameter, in such a way that the cumulative hazard was\n\n$$\nH(t) = \\left(\\frac{t}{b}\\right)^a.\n$$ {#eq-rweibull}\n\nThree different Weibull distributions were used for the event and censoring time generation, yielding a total of 9 combinations. Shape parameters $(a)$ were chosen such that events/censoring occurred early $(a=4)$, evenly spread $(a=1)$, or late $(a=\\frqc{1}{4})$ during follow-up. The scale parameters $(b)$, defining when the cumulative hazard reaches $1$, was set to $b=2.5$ for event simulations, while $b=3.5$ was used for censoring (Figure S1 in Supplement 2). This difference ensures that, on average, more patients will have an event than be censored with the same shape parameter. Each simulated sample consisted of 1,000 patients. The estimated survival function, $N_{\\text{eff, (mod)}}$, the number of patients uncensored and the number at risk were averaged over 50,000 replications for stability. The number at risk is defined as the number of patients who were both alive and still in follow-up. The number not censored is defined as the sum of the number at risk and the number of previous events. \n\n\n\n\n\n\n\n::: {#cell-fig-weibull .cell .hidden}\n\n```{.r .cell-code .hidden}\n# Generate Weibull distributed data\ndata1 <- -log( 1- pweibull(seq(0, 5, by = 0.01), shape = 1/4, scale = 2.5) )\ndata2 <- -log( 1- pweibull(seq(0, 5, by = 0.01), shape = 1, scale = 2.5) )\ndata3 <- -log( 1- pweibull(seq(0, 5, by = 0.01), shape = 4, scale = 2.5) )\ndata4 <- -log( 1- pweibull(seq(0, 5, by = 0.01), shape = 1/4, scale = 3.5) )\ndata5 <- -log( 1- pweibull(seq(0, 5, by = 0.01), shape = 1, scale = 3.5) )\ndata6 <- -log( 1- pweibull(seq(0, 5, by = 0.01), shape = 4, scale = 3.5) )\n#data7 <- -log( 1- pweibull(seq(0, 5, by = 0.01), shape = 0.5, scale = 3) )\n\n# Combine density estimates into a data frame\ndf <- data.frame(\n  x = c(seq(0, 5, by = 0.01), seq(0, 5, by = 0.01), seq(0, 5, by = 0.01), seq(0, 5, by = 0.01), seq(0, 5, by = 0.01), seq(0, 5, by = 0.01)#, seq(0, 5, by = 0.01)\n        ),\n  y = c(data1, data2, data3, data4, data5, data6#, data7\n        ),\n  group = factor(rep(c(\"Early Event\", \"Constant Event\", \"Late Event\", \"Early Censor\", \"Constant Censor\", \"Late Censor\"#, \"Trial Simulation\"\n                       ), \n                     each = length(data1)))\n)\n\n# Define custom colors\ncustom_colors <- c(\"Early Event\" = \"#2E7691\", \n                   \"Constant Event\" = \"#C2666B\", \n                   \"Late Event\" = \"#c6aa2c\",\n                   \"Early Censor\" = \"#86c2d9\",\n                   \"Constant Censor\" = \"#e0b2b5\",\n                   \"Late Censor\" = \"#e2cf7b\"#,\n                   #\"Trial Simulation\" = \"#9f84af\"\n                   )\n\n# Plot using ggplot2\nplot <- ggplot(df, aes(x = x, y = y, color = group)) +\n  geom_line( linewidth = 1.2 ) +\n  scale_color_manual(values = custom_colors) +\n  xlim(0, 5) +\n  ylim(0, 3) +\n  labs(title = \"Comparison of the cumulative hazards for simulation\",\n       x = \"Time (years)\",\n       y = \"Cumulative hazard\",\n       color = \"Simulation\") +\n  theme_minimal()\nplot\n#ggsave(file = \"images/cumhaz.svg\", plot=plot, width = 10, height = 7)\n\nrm(data1, data2, data3, data3, data4, data5, data6, #data7, \n   df, custom_colors)\n```\n:::\n\n\n\n\n\n\n[//]: # (![](images/cumhaz.jpg){#fig-cumhaz fig-alt=\"Cumulative hazards for the simulation scenarios. Early censoring and events use a shape parameter of ¼, causing a sharp rise very early on. Constant censoring and events have shape parameter of 1, making them equivalent to exponential distributions. For the late censoring and events a shape parameter of 4 is used, causing a rapid increase in cumulative hazards at later times.\"})\n\n\n## Quantifying loss of information for early study reporting\nWe investigated the potential role of effective sample size in informing about the loss of information in reporting the results of a cohort study or trial early, with illustration in the colon data. The outcome of interest was the 5-year overall survival in the three arms of the trial. For this illustration, we generated enrollment times and event times on the scale of calendar time of the study (Supplement 3).\\\\\n\nAll analyses were conducted in `R` [@R] version 4.3.1 and specifically the `survival` package [@package-survival]. The functions used to estimate the effective sample size work on the `survfit` object of (stratified) Kaplan-Meier models and can be found on [Github](https://github.com/toby-hackmann/ess-kaplan-meier).\n\n\n\n# Results\n\n## Illustration in colon cancer data\nOverall survival was $46\\%$ [95% condifdence interval 41--51%] at $8$ years in the colon cancer data set with 452 observed events among 929 patients (@fig-colon A). Before any patient was censored, $N_{\\text{eff, (mod)}}$ was 929, equal to the total sample size. After patients were censored, $N_{\\text{eff, (mod)}}$ began to decrease. The number at riskwas much smaller than $N_{\\text{eff, (mod)}}$ at later times $t$ (@fig-colon B). The number not censored decreased earlier than $N_{\\text{eff, (mod)}}$, because of events occuring early rather than censoring.\n\n\n\n\n\n\n::: {#fig-n-eff-km .cell layout-ncol=\"2\"}\n\n```{.r .cell-code .hidden}\nobj <- survfit( Surv( time, status ) ~ 1, data = colon_os ) |>\n  survfit_n()\n\n# Plot the KM\np1 <- plot_km_eff2( obj, both = F, title = \"Kaplan-Meier curve\", xlab = \"Time (years)\")\n#ggsave(file = \"images/colon_km.svg\", plot=p1, width = 9, height = 10)\np1\n\n# Plot the effective N\np2 <- plot_effective_n( obj, mod = T, survival = F, bounds = F, title = \"Effective sample size\", xlab = \"Time (years)\", xmax = 8 )\n#ggsave(file = \"images/colon_effn.svg\", plot=p2, width = 12, height = 10)\np2\n```\n\nKaplan-Meier estimate of the complete data from the illustrative colon dataset. Next to it is the effective sample size and modified effective sample size of the estimates. For comparison the number of patients at risk is included, as well as the number of patients who have not been censored, which could be considered a 'complete case analysis' sample size.\n\n:::\n\n\n\n\n\n\n![Kaplan-Meier estimate of overall survival based on the complete data from the colon dataset (A), with corresponding indicators of sample size (B). Effective sample size (standard or modified) is always higher than the number at risk and only begins to decrease after patients are censored. Almost no censoring occurred before 5 years of follow-up $(t=5)$.](images/colon_ill_3.jpg){#fig-colon}\n\nThe Kaplan-Meier estimated survival curve was constant after \\~8 years (@fig-colon A), yielding a constant estimate of $N_{\\text{eff}}$ in the tail while -by definition- $N_{\\text{eff, mod}}$ decreased with continued censoring of patients. At 8 years, $N_{\\text{eff}}$ for the overall survival estimate was 385 and $N_{\\text{eff, mod}}$ was 349, while the number at risk was only 26. \n\n\n\n\n\n\n\n::: {#cell-fig-tx .cell}\n\n```{.r .cell-code .hidden}\nsurvfit( Surv( time, status) ~ rx, data = colon_os ) |>\n  survfit_n( ) |> \n  plot_km_eff( both = T, mark = T, title = \"Kaplan-Meier with effective sample size\", xlab = \"Time (years)\", legend.pos = c(0.15, 0.5) ) \n```\n\n::: {.cell-output-display}\n![Kaplan-Meier curve of the colon data stratified by treatment arm. Effective sample size provides a more optimistic value of the amount of information that the estimator is based on, when compared to the number at risk. While there seems to be no real difference in censoring patterns, $N_{\\text{eff}}$ of the three curves at $t=8$ are quite different due to the different times at which the last event occured in the three samples. Modified effective sample size is very similar for the three curves at $t=8$. ](manuscript_files/figure-pdf/fig-tx-1.pdf){#fig-tx fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nWe note that $N_{\\text{eff}}$ at the tail of the curve was highly dependent on the timing of the last event in the three treatment groups ($t=8$ @fig-colon B, @fig-tx). The cohort with Lev+5FU has the earliest last event and the highest $N_{\\text{eff}}$ (210) at $t=8$, and the cohort with only Lev has the latest last event and the lowest $N_{\\text{eff}}$ (75). $N_{\\text{eff, mod}}$ avoided this dependency, with more similar effective sample sizes at $t=8$ (75 to 103) than $N_{\\text{eff}}$ (75 to 210, @fig-tx).\n\n\n## Dependence of $N_{\\text{eff}}$ on hazards\nIn the `colon` data used in the above results, most events occurred early in the follow-up and censoring occurred late. In a scenario of early censoring (left column @fig-simulation) $N_{\\text{eff}}$ decreases rapidly. As time continues, $N_{\\text{eff}}$ remains close to the number of uncensored patients. Patients censored early in follow-up decrease $N_{\\text{eff}}$ by around 1 within a few subsequent events but have a lower impact on the tail than late censorings. An approximately linear decrease in effective sample size over time occurs with a constant hazard of censoring (middle column Figure 4). Late censoring (right column @fig-simulation) shows a similar impact on $N_{\\text{eff}}$ as in the colon data. Initially, $N_{\\text{eff}}$ is close to the complete sample size. At later times t, $N_{\\text{eff}}$ decreases drastically, to numbers far below the number of uncensored patients. Late censoring combined with early events shows the largest difference between standard and modified $N_{\\text{eff}}$\n\nThe shape of $N_{\\text{eff}}$ depends on the censoring hazard. How rapidly it will decrease depends on the event hazards. Modified $N_{\\text{eff}}$ will be more conservative than $N_{\\text{eff}}$ when there is late censoring without late events.\n\n\n![Results of the simulations based on the hazards proposed in Figure 1. The simulations are based on 1000 patients per simulation and are the average of 50,000? repetitions. Early, constant and late censoring results are shown in the left, middle and right columns. Event hazards are different between the top, middle and bottom rows. The shape of $N_{\\text{eff}}$ over time depends mostly on the censoring hazard.](images/sim9.jpg){#fig-simulation}\n\n\n\n\n\n\n::: {#cell-fig-simulation .cell .hidden}\n\n```{.r .cell-code .hidden}\n# Seed for consistency\nset.seed(111923)\n\n\nscale = c(2.5, 3.5) # scale[1] for events, scale[2] for censoring\nshape = c(1/4, 1, 4) # shape[1] for early, [2] for middle and [3] for late\niter = 1 # number of iterations\nn = 10000000 # number of patients\nmaxtime = 5 # maxtime\ntime = seq(0, maxtime, 0.002) # timegrid for plot\n\n\nee <- ec <- el <- ce <- cc <- cl <- le <- lc <- ll <- 0\n# Multiple simulations - average 1 at a time\nfor ( i in 1:iter ){\n  # Generate the six datasets of 1000 patients\n  e_early <- simsurv( dist=\"weibull\", \n                         gammas=shape[1], \n                         lambdas=scale[1]^(-shape[1]), \n                         x=data.frame(id = 1:n),\n                         maxt=maxtime)\n  e_middle <- simsurv( dist=\"weibull\", \n                         gammas=shape[2], \n                         lambdas=scale[1]^(-shape[2]), \n                         x=data.frame(id = 1:n),\n                         maxt=maxtime)\n  e_late <- simsurv( dist=\"weibull\", \n                         gammas=shape[3], \n                         lambdas=scale[1]^(-shape[3]), \n                         x=data.frame(id = 1:n),\n                         maxt=maxtime)\n  c_early <- simsurv( dist=\"weibull\", \n                         gammas=shape[1], \n                         lambdas=scale[2]^(-shape[1]), \n                         x=data.frame(id = 1:n),\n                         maxt=maxtime)\n  c_middle <- simsurv( dist=\"weibull\", \n                         gammas=shape[2], \n                         lambdas=scale[2]^(-shape[2]), \n                         x=data.frame(id = 1:n),\n                         maxt=maxtime)\n  c_late <- simsurv( dist=\"weibull\", \n                         gammas=shape[3], \n                         lambdas=scale[2]^(-shape[3]), \n                         x=data.frame(id = 1:n),\n                         maxt=maxtime)\n  \n  # Build the 9 dataframes for the figures\n  early_early <- data.frame(\n    time = pmin(e_early$eventtime, c_early$eventtime),\n    status = ifelse(e_early$eventtime == maxtime & c_early$eventtime == maxtime, 0,\n                    ifelse(e_early$eventtime <= c_early$eventtime, 1, 0))\n  )\n  early_middle <- data.frame(\n    time = pmin(e_early$eventtime, c_middle$eventtime),\n    status = ifelse(e_early$eventtime == maxtime & c_middle$eventtime == maxtime, 0,\n                    ifelse(e_early$eventtime <= c_middle$eventtime, 1, 0))\n  )\n  early_late <- data.frame(\n    time = pmin(e_early$eventtime, c_late$eventtime),\n    status = ifelse(e_early$eventtime == maxtime & c_late$eventtime == maxtime, 0,\n                    ifelse(e_early$eventtime <= c_late$eventtime, 1, 0))\n  )\n  middle_early <- data.frame(\n    time = pmin(e_middle$eventtime, c_early$eventtime),\n    status = ifelse(e_middle$eventtime == maxtime & c_early$eventtime == maxtime, 0,\n                    ifelse(e_middle$eventtime <= c_early$eventtime, 1, 0))\n  )\n  middle_middle <- data.frame(\n    time = pmin(e_middle$eventtime, c_middle$eventtime),\n    status = ifelse(e_middle$eventtime == maxtime & c_middle$eventtime == maxtime, 0,\n                    ifelse(e_middle$eventtime <= c_middle$eventtime, 1, 0))\n  )\n  middle_late <- data.frame(\n    time = pmin(e_middle$eventtime, c_late$eventtime),\n    status = ifelse(e_middle$eventtime == maxtime & c_late$eventtime == maxtime, 0,\n                    ifelse(e_middle$eventtime <= c_late$eventtime, 1, 0))\n  )\n  late_early <- data.frame(\n    time = pmin(e_late$eventtime, c_early$eventtime),\n    status = ifelse(e_late$eventtime == maxtime & c_early$eventtime == maxtime, 0,\n                    ifelse(e_late$eventtime <= c_early$eventtime, 1, 0))\n  )\n  late_middle <- data.frame(\n    time = pmin(e_late$eventtime, c_middle$eventtime),\n    status = ifelse(e_late$eventtime == maxtime & c_middle$eventtime == maxtime, 0,\n                    ifelse(e_late$eventtime <= c_middle$eventtime, 1, 0))\n  )\n  late_late <- data.frame(\n    time = pmin(e_late$eventtime, c_late$eventtime),\n    status = ifelse(e_late$eventtime == maxtime & c_late$eventtime == maxtime, 0,\n                    ifelse(e_late$eventtime <= c_late$eventtime, 1, 0))\n  )\n  ee <- ee + 1/iter*(survfit( Surv( time, status ) ~ 1, data = early_early ) |>\n                       survfit_n() |>\n                       sf_to_df( time = time ))\n  ec <- ec + 1/iter*(survfit( Surv( time, status ) ~ 1, data = early_middle ) |>\n                       survfit_n() |>\n                       sf_to_df( time = time ))\n  el <- el + 1/iter*(survfit( Surv( time, status ) ~ 1, data = early_late ) |>\n                       survfit_n() |>\n                       sf_to_df( time = time ))\n  ce <- ce + 1/iter*(survfit( Surv( time, status ) ~ 1, data = middle_early ) |>\n                       survfit_n() |>\n                       sf_to_df( time = time ))\n  cc <- cc + 1/iter*(survfit( Surv( time, status ) ~ 1, data = middle_middle ) |>\n                       survfit_n() |>\n                       sf_to_df( time = time ))\n  cl <- cl + 1/iter*(survfit( Surv( time, status ) ~ 1, data = middle_late ) |>\n                       survfit_n() |>\n                       sf_to_df( time = time ))\n  le <- le + 1/iter*(survfit( Surv( time, status ) ~ 1, data = late_early ) |>\n                       survfit_n() |>\n                       sf_to_df( time = time ))\n  lc <- lc + 1/iter*(survfit( Surv( time, status ) ~ 1, data = late_middle ) |>\n                       survfit_n() |>\n                       sf_to_df( time = time ))\n  ll <- ll + 1/iter*(survfit( Surv( time, status ) ~ 1, data = late_late ) |>\n                       survfit_n() |>\n                       sf_to_df( time = time ))\n}\n\n\n\n\n# Early-early\nplot1 <- plot_effective_n( ee, mod = T, survival = F, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_x = F, ticks_y_sec = F, ylim = c(0, n) )\n\n# Early-middle\nplot2 <- plot_effective_n( ec, mod = T, survival = F, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_x = F, ticks_y = F, ticks_y_sec = F, ylim = c(0, n) )\n\n# Early-late\nplot3 <- plot_effective_n( el, mod = T, survival = F, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_x = F, ticks_y = F, ylim = c(0, n) )\n\n# Middle-early\nplot4 <- plot_effective_n( ce, mod = T, survival = F, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_x = F, ticks_y_sec = F, ylim = c(0, 1000) ) #+\n  #annotate(\"text\", x = 0, y = 480, label = \"Patients\", angle = 90)\n\n# Middle-middle\nplot5 <- plot_effective_n( cc, mod = T, survival = F, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_x = F, ticks_y = F, ticks_y_sec = F, ylim = c(0, n) )\n\n# Middle-late\nplot6 <- plot_effective_n( cl, mod = T, survival = F, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_x = F, ticks_y = F, ylim = c(0, n) ) #+\n  #annotate(\"text\", x = 5, y = 500, label = \"Incidence\", angle = 270)\n\n# Late-early\nplot7 <- plot_effective_n( le, mod = T, survival = F, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_y_sec = F, ylim = c(0, n) )\n\n# Late-middle\nplot8 <- plot_effective_n( lc, mod = T, survival = F, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_y = F, ticks_y_sec = F, ylim = c(0, n) ) #+\n  #annotate(\"text\", x = 2.5, y = 20, label = \"Time\")\n\n# Late-late\nplot9 <- plot_effective_n( ll, mod = T, survival = F, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_y = F, ylim = c(0, n) )\n\n# Combine plots into list\nplots <- list( plot1, plot2, plot3, plot4, plot5, plot6, plot7, plot8, plot9 )\n\n# Column and row labels\ncol_labels <- c(\"Early censoring\", \"Constant censoring\", \"Late censoring\")\nrow_labels <- c(\"Early events\", \"Constant events\", \"Late events\")\n\n# Create text grobs for column and row labels\ncol_text <- lapply(col_labels, textGrob, gp = gpar(fontsize = 14))\nrow_text <- lapply(row_labels, textGrob, gp = gpar(fontsize = 14), rot = 90)\n\n# Create a sample plot to extract the legend\nlegend_plot <- ggplot(data.frame(x = 1, y = 1, \n                                 color = c(\"Effective\", \"Modified\", \"Uncensored\", \"At risk\", \"Incidence\"), \n                                 linetype = c(\"Effective\", \"Modified\", \"Uncensored\", \"At risk\", \"Incidence\")), \n                      aes(x = x, y = y, color = color, linetype = linetype)) +\n  geom_line(linewidth = 1.3) +\n  scale_color_manual(values = c(\"Effective\"= \"#37293F\", \"Modified\" = \"#9f84af\", \"Uncensored\" = \"#c6aa2c\", \"At risk\" = \"#C2666B\"), name = \"Legend\") +\n  scale_linetype_manual(values = c(\"Effective\"= \"solid\", \"Modified\" = \"solid\", \"Uncensored\" = \"dotdash\", \"At risk\" = \"dashed\"), name = \"Legend\") +\n  theme(legend.position = \"right\") + theme_minimal()\n# \"Bounds\", \n# \"Bounds\" = \"#c6b5cf\", \n# \"Bounds\" = \"dashed\", \n\n\n# Extract the legend\nlegend <- cowplot::get_legend(legend_plot)\n\n# Create top row with column labels\ntop_row <- arrangeGrob(\n  grobs = c(list(textGrob(\"\")), col_text),\n  ncol = 4,\n  widths = c(0.1, 732/675, 1, 732/675)\n)\n\n# Create each row with plots and row label\nrows <- lapply(1:3, function(i) {\n  arrangeGrob(\n    grobs = c(list(row_text[[i]]), plots[((i-1)*3+1):(i*3)]),\n    ncol = 4,\n    widths = c(0.1, 732/675, 1, 732/675)\n  )\n})\n\n# Combine the top row and all plot rows into the final grid\nfinal_grid <- arrangeGrob(\n  grobs = c(list(top_row), rows),\n  ncol = 1,\n  heights = c(0.1, 1, 1, 827/800)\n)\n\n# Add legend to the grid\ncombined_grid <- arrangeGrob(\n  final_grid,\n  legend,\n  ncol = 2,\n  widths= c(8, 1)\n)\n\n\ngrid.arrange(combined_grid)\n\n\n#ggsave(file = \"images/sim2.svg\", plot=combined_grid, width = 12, height = 12, dpi = 700)\nrm( shape, scale, e_early, e_middle, e_late, c_early, c_middle, c_late, \n    early_early, early_middle, early_late, middle_early, middle_middle, \n    middle_late, late_early, late_middle, late_late, plot1, plot2, plot3, plot4,\n    plot5, plot6, plot7, plot8, plot9, plots, col_labels, row_labels, col_text, \n    row_text, legend_plot, legend, top_row, rows, final_grid, combined_grid)\n```\n:::\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\n# This is just to generate seperate KMs\n\n e_early <- simsurv( dist=\"weibull\", \n                         gammas=shape[1], \n                         lambdas=scale[1]^(-shape[1]), \n                         x=data.frame(id = 1:100000),\n                         maxt=maxtime)\n  p <- survfit( Surv( eventtime, status)~1, data = e_early)|>\n        survfit_n() |>\n        plot_km_eff2( xlab = \"Time (years)\")\n  ggsave(file = \"images/sim_km_e.svg\", plot=p, width = 7, height = 5)\n  \n  e_middle <- simsurv( dist=\"weibull\", \n                         gammas=shape[2], \n                         lambdas=scale[1]^(-shape[2]), \n                         x=data.frame(id = 1:100000),\n                         maxt=maxtime)\n    p <- survfit( Surv( eventtime, status)~1, data = e_middle)|>\n        survfit_n() |>\n        plot_km_eff2( xlab = \"Time (years)\")\n  ggsave(file = \"images/sim_km_m.svg\", plot=p, width = 7, height = 5)\n  \n  e_late <- simsurv( dist=\"weibull\", \n                         gammas=shape[3], \n                         lambdas=scale[1]^(-shape[3]), \n                         x=data.frame(id = 1:100000),\n                         maxt=maxtime)\n    p <- survfit( Surv( eventtime, status)~1, data = e_late)|>\n        survfit_n() |>\n        plot_km_eff2( xlab = \"Time (years)\")\n  ggsave(file = \"images/sim_km_l.svg\", plot=p, width = 7, height = 5)\n  \n  c_early <- simsurv( dist=\"weibull\", \n                         gammas=shape[1], \n                         lambdas=scale[2]^(-shape[1]), \n                         x=data.frame(id = 1:100000),\n                         maxt=maxtime)\n    p <- survfit( Surv( eventtime, status)~1, data = c_early)|>\n        survfit_n() |>\n        plot_km_eff2( xlab = \"Time (years)\")\n  ggsave(file = \"images/simc_km_e.svg\", plot=p, width = 7, height = 5)\n  \n  c_middle <- simsurv( dist=\"weibull\", \n                         gammas=shape[2], \n                         lambdas=scale[2]^(-shape[2]), \n                         x=data.frame(id = 1:100000),\n                         maxt=maxtime)\n    p <- survfit( Surv( eventtime, status)~1, data = c_middle)|>\n        survfit_n() |>\n        plot_km_eff2( xlab = \"Time (years)\")\n  ggsave(file = \"images/simc_km_m.svg\", plot=p, width = 7, height = 5)\n  \n  c_late <- simsurv( dist=\"weibull\", \n                         gammas=shape[3], \n                         lambdas=scale[2]^(-shape[3]), \n                         x=data.frame(id = 1:100000),\n                         maxt=maxtime)\n    p <- survfit( Surv( eventtime, status)~1, data = c_late)|>\n        survfit_n() |>\n        plot_km_eff2( xlab = \"Time (years)\")\n  #ggsave(file = \"images/simc_km_l.svg\", plot=p, width = 7, height = 5)\n```\n:::\n\n::: {#cell-fig-sim-same-risk-diff-ess .cell .hidden}\n\n```{.r .cell-code .hidden}\n# Seed for consistency\nset.seed(111923)\n\n# scale[1] for events, scale[2] for censoring 1 and scale[3] for censoring 2\nscale = c(2.5, 2.5, 2.5)\n# shape[1] for events, [2] for censoring 1 and [3] for censoring 2\nshape = c(1, 1/2, 2)\n# number of iterations\niter = 300\n# maxtime\nmaxtime = 5\n# timegrid for plot\ntime = seq(0, maxtime, 0.002)\n\n# So my setup is as follows. For the survival curve, I take the same for both, a Weibull( 1, 2.5 ), which is also an exponential distribution.\n# For the censoring, if I also use scale 2.5 for both of those, then the cumulative hazard for both the events and censoring will be 1 in both\n# simulations, which means that the number a risk must be the same as well (asymptotically, not every iteration of course). Then I can \n# change the effective sample size by varying the shape of the censoring distribution. If I change it too harshly, then the survival\n# function may be affected, so I choose 2 and 1/2 for it.\n\ndat1 <- dat2 <- 0\n# Multiple simulations - average 1 at a time\nfor ( i in 1:iter ){\n  # Generate the six datasets of 1000 patients\n  events <- simsurv( dist=\"weibull\", \n                         gammas=shape[1], \n                         lambdas=scale[1]^(-shape[1]), \n                         x=data.frame(id = 1:1000),\n                         maxt=maxtime)\n  censoring_1 <- simsurv( dist=\"weibull\", \n                         gammas=shape[2], \n                         lambdas=scale[2]^(-shape[2]), \n                         x=data.frame(id = 1:1000),\n                         maxt=maxtime)\n  censoring_2 <- simsurv( dist=\"weibull\", \n                         gammas=shape[3], \n                         lambdas=scale[3]^(-shape[3]), \n                         x=data.frame(id = 1:1000),\n                         maxt=maxtime)\n  \n  # Build the 2 dataframes for the figures\n  df_1 <- data.frame(\n    time = pmin(events$eventtime, censoring_1$eventtime),\n    status = ifelse(events$eventtime == maxtime & censoring_1$eventtime == maxtime, 0,\n                    ifelse(events$eventtime <= censoring_1$eventtime, 1, 0))\n  )\n  df_2 <- data.frame(\n    time = pmin(events$eventtime, censoring_2$eventtime),\n    status = ifelse(events$eventtime == maxtime & censoring_2$eventtime == maxtime, 0,\n                    ifelse(events$eventtime <= censoring_2$eventtime, 1, 0))\n  )\n  dat1 <- dat1 + 1/iter*(survfit( Surv( time, status ) ~ 1, data = df_1 ) |>\n                       survfit_n() |>\n                       sf_to_df( time = time ))\n  dat2 <- dat2 + 1/iter*(survfit( Surv( time, status ) ~ 1, data = df_2 ) |>\n                       survfit_n() |>\n                       sf_to_df( time = time ))\n}\n\nn <- 1000\ncolor_values <- c(\"Early Censoring\"= \"#2E7691\", \"Late Censoring\" = \"#C2666B\")\nlinetype_values <- c(\"Effective\"= \"solid\", \"At Risk\" = \"dashed\")\n# calculate the time where dat1$n.risk and dat2$n.risk cross over, that is, the bigger becomes smaller\nplot <- ggplot() +\n    geom_step(aes(x = dat1$time, y = dat1$n.eff, color = \"Early Censoring\", linetype = \"Effective\"), linewidth = 1.3) +\n    geom_step(aes(x = dat1$time, y = dat1$n.risk, color = \"Early Censoring\", linetype = \"At Risk\"), linewidth = 1.3) +\n    geom_step(aes(x = dat2$time, y = dat2$n.eff, color = \"Late Censoring\", linetype = \"Effective\"), linewidth = 1.3) +\n    geom_step(aes(x = dat2$time, y = dat2$n.risk, color = \"Late Censoring\", linetype = \"At Risk\"), linewidth = 1.3) +\n    labs(x = \"Time\", y = \"Number\") +\n    scale_x_continuous(breaks = seq(0, maxtime, length.out = 5)) +\n    theme_minimal() +\n    theme(legend.position.inside = c(0.1, 0.4), panel.grid.major.x = element_blank()) +\n    theme(text = element_text(size = 12), plot.title = element_text( size = 18, hjust = 0.3)) +\n    scale_color_manual(values = color_values, name = \"Legend\") +\n    scale_linetype_manual(values = linetype_values, name = \"Legend\") + \n    geom_vline(xintercept = 2.5, linetype = \"dotted\", lwd = 1.3)\nplot\nggsave(file = \"images/hein_sim_n.svg\", plot=plot, width = 7, height = 5)\n\nplot2 <- ggplot() +\n    geom_step(aes(x = dat1$time, y = dat1$surv, color = \"Early Censoring\"), linewidth = 1.3) +\n    geom_step(aes(x = dat2$time, y = dat2$surv, color = \"Late Censoring\"), linewidth = 1.3) +\n    labs(x = \"Time\", y = \"Survival probability\") +\n    scale_x_continuous(breaks = seq(0, maxtime, length.out = 5)) +\n    theme_minimal() +\n    theme(legend.position.inside = c(0.1, 0.4), panel.grid.major.x = element_blank()) +\n    theme(text = element_text(size = 12), plot.title = element_text( size = 18, hjust = 0.3)) +\n    scale_color_manual(values = color_values, name = \"Legend\") +\n    scale_linetype_manual(values = linetype_values, name = \"Legend\") + \n    geom_vline(xintercept = 2.5, linetype = \"dotted\", lwd = 1.3)\nplot2\n#ggsave(file = \"images/hein_sim_surv.svg\", plot=plot2, width = 7, height = 5)\n```\n:::\n\n\n\n\n\n\nThe rows in @fig-simulation show that differences in censoring hazard cause different shapes of $N_{\\text{eff}}$ for the survival estimate. There are also differences in the number at risk between these scenarios. However, in a simulation where the survival probability is the exact same (@fig-samerisk), the effective sample size is different when the number at risk is also the same at $t=2.5$ (@fig-samerisk).\n\n![Example where both survival probabilities and number at risk are the same in two data sets at $t = 2.5$. Due to different censoring distributions, the effective sample size is different.](images/same_risk_diff_n.jpg){#fig-samerisk}\n\n## Effective sample size during trial follow-up\nEffective sample size can inform how much information is available for estimating the survival probability at the timepoint of interest. Maximum information is available when all patients have either experienced an event or reached the time horizon of interest. If either happens to all patients, then $N_{\\text{eff}}$ of the estimated survival probability at the time horizon of interest is equal to the number of enrolled patients. \n\n\n\n\n\n\n::: {#cell-fig-sim-trial .cell .hidden}\n\n```{.r .cell-code .hidden}\n# Seed for consistency\nset.seed(111923)\n\n# Scale and Shape\nscale = 3\nshape = 0.5\n\n# number of iterations\niter = 1\n# timegrid for plot\nmaxtime = 2\ntime = seq(0, maxtime, 0.002)\n\n\n# Multiple simulations - average 1 at a time\nfor ( i in 1:iter ){\n  # Generate the survival time\n  survival <- simsurv( dist=\"weibull\", \n                         gammas=shape, \n                         lambdas=scale^(-shape), \n                         x=data.frame(id = 1:250),\n                         maxt=maxtime)\n  \n  # Generate enrollment time\n  survival$tstart <- runif(250, 0, 2)\n  \n  # Survival in 'real' time since start study\n  survival$tstop <- survival$tstart + survival$eventtime\n  \n  \n  four <- data.frame(\n  time = survival$eventtime,\n  status = survival$status\n  )\n  \n  three <- data.frame(\n  time = ifelse( survival$tstop < 3, survival$eventtime, survival$eventtime - (survival$tstop - 3)),\n  status = ifelse( survival$tstop < 3, survival$status, 0)\n  )\n\n  two <- data.frame(\n  time = ifelse( survival$tstop < 2, survival$eventtime, survival$eventtime - (survival$tstop - 2)),\n  status = ifelse( survival$tstop < 2, survival$status, 0)\n  )\n}\n\n# Two years\nplot1 <- survfit( Surv(time, status) ~1, data = two ) |> \n  survfit_n() |>\n  plot_effective_n( mod = T, survival = T, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_x = T, ticks_y_sec = F )\n\n# Two half years\nplot2 <- survfit( Surv(time, status) ~1, data = three ) |> \n  survfit_n() |>\n  plot_effective_n( mod = T, survival = T, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_x = T, ticks_y_sec = F )\n\n# Three years\nplot3 <- survfit( Surv(time, status) ~1, data = four ) |> \n  survfit_n() |>\n  plot_effective_n( mod = T, survival = T, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_x = T, ticks_y_sec = F )\n\n# Combine plots into list\nplots <- list( plot1, plot2, plot3 )\n\n# Column and row labels\ncol_labels <- c(\"Two years\", \"Three years\", \"Four years\")\n\n# Create text grobs for column and row labels\ncol_text <- lapply(col_labels, textGrob, gp = gpar(fontsize = 12))\n\n# Create a sample plot to extract the legend\nlegend_plot <- ggplot(data.frame(x = 1, y = 1, \n                                 color = c(\"Effective\", \"Modified\", \"Uncensored\", \"At risk\", \"Incidence\"), \n                                 linetype = c(\"Effective\", \"Modified\", \"Uncensored\", \"At risk\", \"Incidence\")), \n                      aes(x = x, y = y, color = color, linetype = linetype)) +\n  geom_line(linewidth = 1.3) +\n  scale_color_manual(values = c(\"Effective\"= \"#37293F\", \"At risk\" = \"#C2666B\", \"Uncensored\" = \"#2E7691\", \"Modified\" = \"#9f84af\", \"Incidence\" = \"#c6aa2c\"), name = \"Legend\") +\n  scale_linetype_manual(values = c(\"Effective\"= \"solid\", \"At risk\" = \"longdash\", \"Uncensored\" = \"dotdash\", \"Modified\" = \"solid\", \"Incidence\" = \"solid\"), name = \"Legend\") +\n  theme(legend.position = \"right\") + theme_minimal()\n# \"Bounds\", \n# \"Bounds\" = \"#c6b5cf\", \n# \"Bounds\" = \"dashed\", \n\n\n# Extract the legend\nlegend <- cowplot::get_legend(legend_plot)\n\n# Create top row with column labels\ntop_row <- arrangeGrob(\n  grobs = col_text,\n  ncol = 3,\n  widths = c(732/675, 1, 732/675)\n)\n\n# Create each row with plots and row label\nrows <- arrangeGrob(\n    grobs = plots[1:3],\n    ncol = 3,\n    widths = c(732/675, 1, 732/675)\n)\n\n# Combine the top row and all plot rows into the final grid\nfinal_grid <- arrangeGrob(\n  grobs = c(list(top_row), list(rows)),\n  ncol = 1,\n  heights = c(0.1, 1)\n)\n\n# Add legend to the grid\ncombined_grid <- arrangeGrob(\n  final_grid,\n  legend,\n  ncol = 2,\n  widths= c(7, 1)\n)\n\n# Display the final grid\ngrid.draw(combined_grid)\n```\n:::\n\n::: {#cell-fig-sim-reporting .cell}\n\n```{.r .cell-code .hidden}\n# Seed for consistency\nset.seed(111923)\n\nplot <- trial_report( gammas = c(0.5), \n              lambdas = c(3^(-0.5)), \n              x = data.frame( id = 1:500,\n                              treat = c(rep(0, 250), rep(1, 250))),\n              betas = c(treat = 0.7),\n              t_outcome = 2, \n              t_enroll = 2, \n              iter = 250, \n              n_patient = 250,\n              points = 25,\n              conf = T,\n              conf.int = 95,\n              proportion = F,\n              mod = F,\n              ylab = \"Effective Sample Size for 2 year survival\",\n              xlab = \"Time since study start\",\n              title = \"Effective sample size over reporting time\"\n              )\nplot\n#ggsave(file = \"images/trial_std.svg\", plot=plot, width = 7, height = 5)\n```\n:::\n\n\n\n\n\n\n![Percentage of modified Neff of the two-year Kaplan-Meier survival estimate as a function of time. A treatment was given to group Treat 2 with a hazard ratio of 0.7. Pointwise confidence intervals are based on the observed 95% range (2.5-97.5% interquantile range) based on 25000 simulations. Most of the information in the trial is already available one year before the natural end of the trial. ](images/trial%.jpg){#fig-trial}\n\nThis simulation shows that 87% of the information is available for the untreated group (`Treat 1`) at reporting time of three yearsThe group receiving the treatment (`Treat 2`) has a lower effective sample size at most reporting times. For the treated group, the effective sample size is around 83% (72%-91%) of the maximum at a reporting time of 3 years of study duration.\n\n\n# Discussion\nSample size is a key concept in drawing conclusions from data [@consort]: an estimate based on a larger sample size is more reliable than one based on a smaller sample size, all else staying the same. In the context of time-to-event data sample size is a more challenging concept, because of censoring. To inform about the reliability of an estimate with the same interpretation as the sample size in settings with complete follow-up, we argue for use of the effective sample size for several applications.\n\n\n## Presented below Kaplan-Meier\nKaplan-Meier curves are often presented to show the primary outcome of a study, such as overall survival, or relapse-free survival. Guidelines and tutorials on the presentation of survival curves state that the number of patients at risk should be presented below the curve [@vickers2020; @morris2019; @jager2008; @pocock2002]. Often this is accompanied by a recommendation that the survival curve is truncated when the number at risk falls below a certain percentage of the number non censored (10-20%) [@pocock2002], or below an absolute number (5-10) [@vickers2020]. Number at risk is a measure of the reliability of the hazard at time $t$, which relates to the uncertainty in future survival probability estimates, conditional on being alive at time $t$. In contrast, (modified) $N_{\\text{eff}}$ indicates the reliability of the estimated survival probability at time $t$, which is informative from the start of a study.\nEffective sample size can be used in addition to the number of patients at risk in a table below the curve. It can be interpreted as the size of a cohort of $N_{\\text{eff}}$ patients with full follow-up to estimate survival at time $t$ that leads to the same estimated variance.\n\n\n## Trial monitoring\nTrials with a survival outcome are generally powered for detecting an expected difference between at least two arms, often expressed as a hazard ratio. During the design phase, this power analysis results in a required sample size for detecting the expected results with a pre-determined power. During the trial, there may be differences between the numbers used in power calculations during the design phase and what is observed. Such changes are not always acted upon, because interim analyses can inflate type I error rates [@houwelingen2005]. This may not applicable to decisions based on estimates of effective sample size, especially if treatment allocation remains blinded to the analyst. \nSeveral situations could be considered where changes in the moment of analysis may be desired. More events than expected can positively impact the effective sample size which may allow for earlier analysis, while more censoring than expected may negatively impact $N_{\\text{eff}}$. Enrollment may also be slower than expected, and lead to constraints with time and funding. In such situations, effective sample size may help answer such questions, but must be used carefully and are not a substitute for power analysis. Further research is required to assess any possible impact on type I error rates from stopping rules based on effective sample size.\n\n\n## Risk communication\nCommunicating with patients about the uncertainty or reliability of the evidence supporting a treatment choice. Open communication about uncertainty is an important step in improving trustworthiness [@spiegelhalter2017;@spiegelhalter2017address]. Experts, such as care givers, are still hesitant to communicate uncertainty, because it may undermine trust in their work [@fischhoff2012] or it seems futile to try to explain it [@kattan2011]. There are indications that communication of uncertainty in a numerical manner does not reduce trust in the number or the source [@vanderbles2020]. Sharing information about the quality of evidence affects decision-making and people are less likely to base their decisions on number with low quality of evidence [@schneider2022].\n\nAn instruction with a nomogram for prostate cancer risk states that physicians should tell a patient: “if we had 100 men exactly like you, we would expect between $risk\\%-10\\%$ and $risk\\%+10\\%$ to remain free of their disease at 5 years” [@kattan2002]. Such a presentation conveys the absolute risk to a patient. It does not tell the patient how many ‘men like you’ this number was based on, which is provided by the effective sample size. Note that there is also no explanation given of what ‘men exactly like you’ means, which is therefore left open to interpretation. Future research into effective sample size for Cox proportional hazards prediction models could develop the concept of effective sample size into a number more closely resembling the idea of ‘patients like you’. This would be the effective sample size of a prediction $\\hat S(t, x_{new})$ for a new patient with covariates $x_{new}$ (Thomassen 2024).  \n\nSuch communication about uncertainty could be used in the framework of shared decision making in medical care [@stiggelbout2015;@4Dpicture]. We provide an illustrative example of a communication tool based on the colon data with effective sample size in @fig-example. The definition of ‘patients like you’ for effective sample size are patients with the same values for the model covariates, in this example  only treatment. For clinical application, important prognostic factors should be considered, and the format for presentation may need further refinement.\n\n\n![Example of using effective sample size in risk communication. This example compares the survival in the arm of observation after surgery to that of adjuvant Lev+5FU after surgery. It illustrates a possible visualization of the risk difference and communication of effective sample size.](images/communication.jpg){#fig-example}\n\nWe note that uncertainty communication using effective sample size may not be  preferred by all patients. Patients with high numeracy and statistical literacy may prefer to learn the $95\\%$ confidence interval for a prediction. Patients with low numeracy may prefer to only know if an estimate is certain or uncertain [@vromans2021].\n\n\n# Conclusion\nEffective sample size is valuable as an intuitive measure of the uncertainty of a survival estimate from the Kaplan-Meier estimator, which has the same interpretation as sample size in studies without censoring. It can be used in risk tables in addition to the number at risk, where it is an intuitive estimate of reliability for estimated survival probabilities at the time of interest. Future research should explore the role of effective sample size for trial monitoring, uncertainty communication and regression models with survival outcomes.\n",
    "supporting": [
      "manuscript_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}