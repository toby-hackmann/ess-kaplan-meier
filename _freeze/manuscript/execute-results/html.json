{
  "hash": "aed9f5d42a682598e5c4805709d0e6cc",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Effective sample size for the Kaplan-Meier estimator: An intuitive measure of uncertainty\"\nauthors:\n  - name: Toby Hackmann\n    affiliation: \n      - ref: lumc\n    roles: analysis, writing\n    corresponding: true\n  - name: Doranne Thomassen\n    affiliation: \n      - ref: lumc\n    roles: conceptualization\n  - name: Anne M Stiggelbout,\n    affiliation: \n      - ref: lumc\n  - name: Saskia le Cessie\n    affiliation:\n      - ref: lumc\n      - ref: epi\n  - name: Hein Putter\n    affiliation: \n      - ref: lumc\n  - name: Liesbeth C de Wreede\n    affiliation: \n      - ref: lumc\n  - name: Ewout W Steyerberg\n    affiliation: \n      - ref: lumc\n      - ref: julius\n    \naffiliations:\n  - id: lumc\n    name: Department of Biomedical Data Sciences, Leiden University Medical Center, Leiden, the Netherlands\n  - id: epi\n    name: Department of Clinical Epidemiology, Leiden University Medical Center, Leiden, the Netherlands\n  - id: julius\n    name: Julius Center for Health Sciences and Primary Care, University Medical Center Utrecht, Utrecht, the Netherlands\n    \nbibliography: references.bib\n\nabstract: | \n    Sample size is an essential indicator of the uncertainty in clinical research results. When studies present time-to-event outcomes with Kaplan-Meier curves, these are often accompanied by the remaining number of patients at risk in a table below the curve. The number at risk at time t informs about uncertainty of the hazard at t, rather than the uncertainty of the estimated survival probability until t,S ̂(t). We aim to review the role of the effective sample size of S ̂(t) to reflect the uncertainty in survival probability estimation. Effective sample size is defined as the size of a hypothetical sample with complete follow-up until time t, that would give the same variance as the variance of the Kaplan-Meier estimate S ̂(t). Illustrations in hypothetical scenarios and in a publicly available dataset support that effective sample size provides a readily interpretable measure of uncertainty for survival curves in the presence of censoring. We show that effective sample size can also quantify the loss of information when reporting for an ongoing study is moved to an earlier time point. Effective sample size is a valuable measure that could be used more often in survival analysis.\n   \nkeywords:\n  - sample size\n  - kaplan-meier\n  - survival\n  - risk communication\n  \nfunding: \"Funded by the European Union under Horizon Europe Work Programme 101057332. Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Health and Digital Executive Agency (HaDEA). Neither the European Union nor the granting authority can be held responsible for them. The UK team are funded under the Innovate UK Horizon Europe Guarantee Programme, UKRI Reference Number: 10041120.\"\n---\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\n# To reproduce the results\nset.seed(19231030)\n\n# Packages required\npkgs <- c(\n  \"grid\",\n  \"gridExtra\",\n  \"plotly\"\n)\n\n# Check if the package is installed, if yes load, if no: install + load \nvapply(pkgs, function(pkg) {\n  if (!require(pkg, character.only = TRUE)) install.packages(pkg)\n  require(pkg, character.only = TRUE, quietly = TRUE)\n}, FUN.VALUE = logical(length = 1L))\n\nrm(pkgs)\n\nsource( \"code/compile.R\" )\n\n# load data\ncolon_df <- colon\nnames <- c('sex', 'obstruct', 'perfor', 'differ', 'extent', 'surg', 'node4', 'etype')\ncolon_df <- colon_df |> \n  mutate( across( all_of(names), as.numeric ) )\nrm(names)\ncolon_os <- colon_df[colon_df$etype == 2, ]\ncolon_rfs <- colon_df[colon_df$etype == 1, ]\n\ncolon_os$time <- colon_os$time/365.25\n```\n:::\n\n\n\n\n\n\n# Introduction\nUncertainty in results from clinical studies is lower with larger sample sizes. Sample size hence is an intuitive measure of uncertainty [@sedlmeier1997]. For survival, the time of the event of interest is typically not observed for all study participants (‘censoring’). Censoring may occur for two main reasons: patients can drop out of the study (‘lost to follow-up’), or they can be administratively censored when the study observation period ends. In both cases, a common assumption in survival analysis is that censoring at time t provides no additional information about prognosis after $t$ [@fojo2021]. In medical research and related fields, the Kaplan-Meier product limit estimator is widely used to provide survival probabilities over time while respecting censoring [@kaplanmeier1958]. Kaplan-Meier curves are often accompanied by the remaining number of patients at risk in a table below the curve. The number at risk at time $t$ informs about uncertainty of the hazard at $t$, and survival after $t$. \n\nIn the current paper we focus on the uncertainty of the estimated survival probability until $t$. We aim to examine the value of ‘effective sample size’ for this uncertainty. Effective sample size is defined as the sample size equivalent to the sample size of a study without censoring [@kaplanmeier1958]. Although, the concept of effective sample size has been discussed with varying definitions and purposes [@cutler1947; @cutler1958; @peto1977; @rothman1978; @dorey1987; @meier2004], it has not often been used as a measure to communicate the uncertainty of estimated survival probabilities. We illustrate effective sample size in a case study and in simulations to better understand its behavior and potential roles in survival analysis. We end with a discussion on utility of the effective sample size and future research.\n\n\n\n# Methods\n\n## Definition\nEffective sample size has been defined by @kaplanmeier1958 as a sample size, “which in the absence of losses would give the same variance [as the variance of the Kaplan-Meier estimate]”. Suppose we have estimated the survival probability at the time of interest $\\hat S(t)$ using the Kaplan-Meier estimate. The variance of $\\hat S(t)$ can be consistently estimated by Greenwood's formula [@greenwood1926; @andersen1993]. The effective sample size at time $t$ is defined as the size of a hypothetical sample with complete follow-up (no censoring) until time $t$, with mean survival  $\\bar S(t)$ equal to $\\hat S(t)$, such that the variance of $\\hat S(t)$ is the same as the sampling variance of  $\\bar S(t)$ in the hypothetical population. To calculate this effective sample size, we equate the estimated variance of the Kaplan-Meier estimated survival outcome $\\hat S(t)$ with the binomial sampling variance of the mean outcome $\\bar S(t)$. \n\n$$\n\\hat V_{GW}[\\hat S(t) ]  = V_{Bin}[ \\bar S(t) ],\n$$ {#eq-km-greenwood}\nwhich yields\n$$\n\\hat S ^2(t)\\sum_{t_i \\leq t}\\frac{\\delta_i}{R_i(R_i-\\delta_i)} = \\frac{\\bar S(t)(1-\\bar S(t))}{N}, \n$$ {#eq-equiv}\n\nwith $t_1 < \\dots <t_D$ the $D$ distinct times where events are observed and $\\delta_i$ are the number of events and $R_i$ the number of patients at risk at time $t_i$. The Greenwood estimator for the variance of the Kaplan-Meier estimator is used on the left hand side [@greenwood1926]. We replace the sample mean survival with the estimated survival, and the complete sample size with the effective sample size. Now @eq-equiv can be rewritten to provide the estimator for effective sample size:\n$$\n\\hat N_{\\text{eff}}(t) = \\frac{1-\\hat S(t)}{\\hat S (t)\\sum_{i: t_i \\leq t}\\frac{\\delta_i}{R_i(R_i-\\delta_i)}}.\n$$ {#eq-effective-n}\n\nThe estimator for effective sample size is undefined before the first event since $\\sum_{i: t_i \\leq t}\\frac{\\delta_i}{R_i(R_i-\\delta_i)} = 0$. Before the first event, we define effective sample size as the number of patients at risk of an event.\nThis quantity is the ‘effective sample size for estimates based on the Kaplan-Meier estimator’, but for brevity and readability we refer to $N_{\\text{eff}}$ for effective sample size.\n\n\n## Modified effective sample size\nThe Kaplan-Meier survival estimate with Greenwood variance and resulting confidence intervals only change at times with events. After the last event the standard $N_{\\text{eff}}$ estimator (@eq-effective-n) is constant at time points where no events occur but patients are censored. We consider this a limitation of the above definition of $N_{\\text{eff}}$, because, intuitively, censoring should decrease the effective sample size. Methods have been proposed to improve the variance and confidence interval estimation in the tail [@peto1977].  One such method is to modify the variance estimation procedure when there is censoring but no event [@dorey1987]. This method provides better coverage but is also more conservative than the standard Greenwood estimator [@yuan2011].\n\nUsing the modified variance suggested by @dorey1987, instead of Greenwood’s variance, we derive a modified effective sample size $N_{\\text{eff, mod}}$ (mathematical derivation in Supplement 1). $N_{\\text{eff, mod}}$ decreases at times $t$ with only censoring and therefore decreases after the last observed event. We refer to both effective sample size and modified effective sample size with $N_{\\text{eff, (mod)}}$.\n\n\n## Illustrative clinical data\nWe illustrate the behavior of effective sample size using a publicly available colon cancer data set with the `survival` package in `R` [@package-survival]. The data set includes patients from a clinical trial of adjuvant chemotherapy after a resection with curative intent for a histologically confirmed adenocarcinoma of the colon or rectum [@laurie1989; @moertel1995]. Patients were randomized to either observation (Obs), treatment with levimasole (Lev) alone, or treatment with levimasole and fluorouracil (Lev+5FU). Overall survival and recurrence-free survival were end points. We focused our analyses on overall survival.\n\n\n## Simulation study: Dependence on hazard functions\nWe examined the behavior of $N_{\\text{eff, (mod)}}$ further in some simulated settings. In the first simulation we varied hazards for events and censoring in the `simsurv` package [@package-simsurv]. We generated survival times $T_E$ and censoring times $T_C$ from Weibull hazard functions. An event was observed at time $T_E$ if $T_E\\leq T_C$ and a patient was censored at time $T_C$ if $T_C<T_E$. All patients were administratively censored at $t=5$.  We generated survival data using Weibull$(a , b)$ distributions, with $a$ representing the shape parameter, and $b$ the scale parameter, in such a way that the cumulative hazard was\n\n$$\nH(t) = \\left(\\frac{t}{b}\\right)^a.\n$$ {#eq-rweibull}\n\nThree different Weibull distributions were used for the event and censoring time generation, yielding a total of 9 combinations. Shape parameters $(a)$ were chosen such that events/censoring occurred early $(a=4)$, evenly spread $(a=1)$, or late $(a=\\frac{1}{4})$ during follow-up. The scale parameters $(b)$, defining when the cumulative hazard reaches $1$, was set to $b=2.5$ for event simulations, while $b=3.5$ was used for censoring (Figure S1 in Supplement 2). This difference ensures that, on average, more patients will have an event than be censored with the same shape parameter. Each simulated sample consisted of 1,000 patients. The estimated survival function, $N_{\\text{eff, (mod)}}$, the number of patients uncensored and the number at risk were averaged over 50,000 replications for stability. The number at risk is defined as the number of patients who were both alive and still in follow-up. The number not censored is defined as the sum of the number at risk and the number of previous events. \n\n\n\n\n\n\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\n# Generate Weibull distributed data\ndata1 <- -log( 1- pweibull(seq(0, 5, by = 0.01), shape = 1/4, scale = 2.5) )\ndata2 <- -log( 1- pweibull(seq(0, 5, by = 0.01), shape = 1, scale = 2.5) )\ndata3 <- -log( 1- pweibull(seq(0, 5, by = 0.01), shape = 4, scale = 2.5) )\ndata4 <- -log( 1- pweibull(seq(0, 5, by = 0.01), shape = 1/4, scale = 3.5) )\ndata5 <- -log( 1- pweibull(seq(0, 5, by = 0.01), shape = 1, scale = 3.5) )\ndata6 <- -log( 1- pweibull(seq(0, 5, by = 0.01), shape = 4, scale = 3.5) )\n#data7 <- -log( 1- pweibull(seq(0, 5, by = 0.01), shape = 0.5, scale = 3) )\n\n# Combine density estimates into a data frame\ndf <- data.frame(\n  x = c(seq(0, 5, by = 0.01), seq(0, 5, by = 0.01), seq(0, 5, by = 0.01), seq(0, 5, by = 0.01), seq(0, 5, by = 0.01), seq(0, 5, by = 0.01)#, seq(0, 5, by = 0.01)\n        ),\n  y = c(data1, data2, data3, data4, data5, data6#, data7\n        ),\n  group = factor(rep(c(\"Early Event\", \"Constant Event\", \"Late Event\", \"Early Censor\", \"Constant Censor\", \"Late Censor\"#, \"Trial Simulation\"\n                       ), \n                     each = length(data1)))\n)\n\n# Define custom colors\ncustom_colors <- c(\"Early Event\" = \"#2E7691\", \n                   \"Constant Event\" = \"#C2666B\", \n                   \"Late Event\" = \"#c6aa2c\",\n                   \"Early Censor\" = \"#86c2d9\",\n                   \"Constant Censor\" = \"#e0b2b5\",\n                   \"Late Censor\" = \"#e2cf7b\"#,\n                   #\"Trial Simulation\" = \"#9f84af\"\n                   )\n\n# Plot using ggplot2\nplot <- ggplot(df, aes(x = x, y = y, color = group)) +\n  geom_line( linewidth = 1.2 ) +\n  scale_color_manual(values = custom_colors) +\n  xlim(0, 5) +\n  ylim(0, 3) +\n  labs(title = \"Comparison of the cumulative hazards for simulation\",\n       x = \"Time (years)\",\n       y = \"Cumulative hazard\",\n       color = \"Simulation\") +\n  theme_minimal()\nplot\n#ggsave(file = \"images/cumhaz.svg\", plot=plot, width = 10, height = 7)\n\nrm(data1, data2, data3, data3, data4, data5, data6, #data7, \n   df, custom_colors)\n```\n:::\n\n\n\n\n\n\n\n\n## Quantifying loss of information for early study reporting\nWe investigated the potential role of effective sample size in informing about the loss of information in reporting the results of a cohort study or trial early, with illustration in the colon data. The outcome of interest was the 5-year overall survival in the three arms of the trial. For this illustration, we generated enrollment times and event times on the scale of calendar time of the study (Supplement 3).\n\nAll analyses were conducted in `R` [@R] version 4.3.1 and specifically the `survival` package [@package-survival]. The functions used to estimate the effective sample size work on the `survfit` object of (stratified) Kaplan-Meier models and can be found on [Github](https://github.com/toby-hackmann/ess-kaplan-meier).\n\n\n\n# Results\n\n## Illustration in colon cancer data\nOverall survival was $46\\%$ [95% condifdence interval 41--51%] at $8$ years in the colon cancer data set with 452 observed events among 929 patients (@fig-colon A). Before any patient was censored, $N_{\\text{eff, (mod)}}$ was 929, equal to the total sample size. After patients were censored, $N_{\\text{eff, (mod)}}$ began to decrease. The number at risk was much smaller than $N_{\\text{eff, (mod)}}$ at later times $t$ (@fig-colon B). The number not censored decreased earlier than $N_{\\text{eff, (mod)}}$, because of events occuring early rather than censoring.\n\n\n\n\n\n\n::: {#fig-n-eff-km .cell .hidden layout-ncol=\"2\"}\n\n```{.r .cell-code .hidden}\nobj <- survfit( Surv( time, status ) ~ 1, data = colon_os ) |>\n  survfit_n()\n\n# Plot the KM\np1 <- plot_km_eff2( obj, both = F, title = \"Kaplan-Meier curve\", xlab = \"Time (years)\")\n#ggsave(file = \"images/colon_km.svg\", plot=p1, width = 9, height = 10)\np1\n\n# Plot the effective N\np2 <- plot_effective_n( obj, mod = T, survival = F, bounds = F, title = \"Effective sample size\", xlab = \"Time (years)\", xmax = 8 )\n#ggsave(file = \"images/colon_effn.svg\", plot=p2, width = 12, height = 10)\np2\n```\n\nKaplan-Meier estimate of the complete data from the illustrative colon dataset. Next to it is the effective sample size and modified effective sample size of the estimates. For comparison the number of patients at risk is included, as well as the number of patients who have not been censored, which could be considered a 'complete case analysis' sample size.\n\n:::\n\n\n\n\n\n\n![Kaplan-Meier estimate of overall survival based on the complete data from the colon dataset (A), with corresponding indicators of sample size (B). Effective sample size (standard or modified) is always higher than the number at risk and only begins to decrease after patients are censored. Almost no censoring occurred before 5 years of follow-up $(t=5)$.](images/colon_ill_3.jpg){#fig-colon}\n\nThe Kaplan-Meier estimated survival curve was constant after \\~8 years (@fig-colon A), yielding a constant estimate of $N_{\\text{eff}}$ in the tail while -by definition- $N_{\\text{eff, mod}}$ decreased with continued censoring of patients. At 8 years, $N_{\\text{eff}}$ for the overall survival estimate was 385 and $N_{\\text{eff, mod}}$ was 349, while the number at risk was only 26. \n\n\n![Kaplan-Meier estimate of overall survival based on the complete data from the colon dataset (A), with corresponding indicators of sample size (B). Effective sample size (standard or modified) is always higher than the number at risk and only begins to decrease after patients are censored. Almost no censoring occurred before 5 years of follow-up $(t=5)$.](images/km_tx.jpg){#fig-tx}\n\n\n\n\n\n::: {#cell-fig-tx-auto .cell}\n\n```{.r .cell-code .hidden}\nsurvfit( Surv( time, status) ~ rx, data = colon_os ) |>\n  survfit_n( ) |> \n  plot_km_eff( both = T, mark = T, title = \"Kaplan-Meier with effective sample size\", xlab = \"Time (years)\", legend.pos = c(0.15, 0.5) ) \n```\n:::\n\n\n\n\n\n\nWe note that $N_{\\text{eff}}$ at the tail of the curve was highly dependent on the timing of the last event in the three treatment groups ($t=8$ @fig-colon B, @fig-tx). The cohort with Lev+5FU has the earliest last event and the highest $N_{\\text{eff}}$ (210) at $t=8$, and the cohort with only Lev has the latest last event and the lowest $N_{\\text{eff}}$ (75). $N_{\\text{eff, mod}}$ avoided this dependency, with more similar effective sample sizes at $t=8$ (75 to 103) than $N_{\\text{eff}}$ (75 to 210, @fig-tx).\n\n\n## Dependence of $N_{\\text{eff}}$ on hazards\nIn the simulation scenario of early censoring (left column @fig-simulation) $N_{\\text{eff, (mod)}}$ decreased more rapidly. As time continued, $N_{\\text{eff, (mod)}}$ remained close to the number of uncensored patients. Patients censored early during follow-up had a large impact on $N_{\\text{eff, (mod)}}$, which decreased by approximately 1 per censored patient, while the impact of censoring on $N_{\\text{eff, (mod)}}$ in the tail was lower. In the simulation scenario with a constant hazard of censoring an approximately linear decrease in effective sample size over time occurred (middle column @fig-simulation). The late censoring scenario (right column @fig-simulation), resembled the colon data with most events occurring early in the follow-up. Initially, $N_{\\text{eff, (mod)}}$ was close to the complete sample size. At later times $t$, $N_{\\text{eff, (mod)}}$ decreased drastically, to numbers far below the number of uncensored patients. Late censoring combined with early events showed the largest difference between $N_{\\text{eff}}$  and $N_{\\text{eff, mod}}$. \n\n\n![Results of the simulations based on the hazards proposed in Figure 1. The simulations are based on 1000 patients per simulation and are the average of 50,000? repetitions. Early, constant and late censoring results are shown in the left, middle and right columns. Event hazards are different between the top, middle and bottom rows. The shape of $N_{\\text{eff}}$ over time depends mostly on the censoring hazard.](images/sim9.jpg){#fig-simulation}\n\n\n\n\n\n\n::: {#cell-fig-simulation .cell .hidden}\n\n```{.r .cell-code .hidden}\n# Seed for consistency\nset.seed(111923)\n\n\nscale = c(2.5, 3.5) # scale[1] for events, scale[2] for censoring\nshape = c(1/4, 1, 4) # shape[1] for early, [2] for middle and [3] for late\niter = 10 # number of iterations\nn = 1000 # number of patients\nmaxtime = 5 # maxtime\ntime = seq(0, maxtime, 0.002) # timegrid for plot\n\n\nee <- ec <- el <- ce <- cc <- cl <- le <- lc <- ll <- 0\n# Multiple simulations - average 1 at a time\nfor ( i in 1:iter ){\n  # Generate the six datasets of 1000 patients\n  e_early <- simsurv( dist=\"weibull\", \n                         gammas=shape[1], \n                         lambdas=scale[1]^(-shape[1]), \n                         x=data.frame(id = 1:n),\n                         maxt=maxtime)\n  e_middle <- simsurv( dist=\"weibull\", \n                         gammas=shape[2], \n                         lambdas=scale[1]^(-shape[2]), \n                         x=data.frame(id = 1:n),\n                         maxt=maxtime)\n  e_late <- simsurv( dist=\"weibull\", \n                         gammas=shape[3], \n                         lambdas=scale[1]^(-shape[3]), \n                         x=data.frame(id = 1:n),\n                         maxt=maxtime)\n  c_early <- simsurv( dist=\"weibull\", \n                         gammas=shape[1], \n                         lambdas=scale[2]^(-shape[1]), \n                         x=data.frame(id = 1:n),\n                         maxt=maxtime)\n  c_middle <- simsurv( dist=\"weibull\", \n                         gammas=shape[2], \n                         lambdas=scale[2]^(-shape[2]), \n                         x=data.frame(id = 1:n),\n                         maxt=maxtime)\n  c_late <- simsurv( dist=\"weibull\", \n                         gammas=shape[3], \n                         lambdas=scale[2]^(-shape[3]), \n                         x=data.frame(id = 1:n),\n                         maxt=maxtime)\n  \n  # Build the 9 dataframes for the figures\n  early_early <- data.frame(\n    time = pmin(e_early$eventtime, c_early$eventtime),\n    status = ifelse(e_early$eventtime == maxtime & c_early$eventtime == maxtime, 0,\n                    ifelse(e_early$eventtime <= c_early$eventtime, 1, 0))\n  )\n  early_middle <- data.frame(\n    time = pmin(e_early$eventtime, c_middle$eventtime),\n    status = ifelse(e_early$eventtime == maxtime & c_middle$eventtime == maxtime, 0,\n                    ifelse(e_early$eventtime <= c_middle$eventtime, 1, 0))\n  )\n  early_late <- data.frame(\n    time = pmin(e_early$eventtime, c_late$eventtime),\n    status = ifelse(e_early$eventtime == maxtime & c_late$eventtime == maxtime, 0,\n                    ifelse(e_early$eventtime <= c_late$eventtime, 1, 0))\n  )\n  middle_early <- data.frame(\n    time = pmin(e_middle$eventtime, c_early$eventtime),\n    status = ifelse(e_middle$eventtime == maxtime & c_early$eventtime == maxtime, 0,\n                    ifelse(e_middle$eventtime <= c_early$eventtime, 1, 0))\n  )\n  middle_middle <- data.frame(\n    time = pmin(e_middle$eventtime, c_middle$eventtime),\n    status = ifelse(e_middle$eventtime == maxtime & c_middle$eventtime == maxtime, 0,\n                    ifelse(e_middle$eventtime <= c_middle$eventtime, 1, 0))\n  )\n  middle_late <- data.frame(\n    time = pmin(e_middle$eventtime, c_late$eventtime),\n    status = ifelse(e_middle$eventtime == maxtime & c_late$eventtime == maxtime, 0,\n                    ifelse(e_middle$eventtime <= c_late$eventtime, 1, 0))\n  )\n  late_early <- data.frame(\n    time = pmin(e_late$eventtime, c_early$eventtime),\n    status = ifelse(e_late$eventtime == maxtime & c_early$eventtime == maxtime, 0,\n                    ifelse(e_late$eventtime <= c_early$eventtime, 1, 0))\n  )\n  late_middle <- data.frame(\n    time = pmin(e_late$eventtime, c_middle$eventtime),\n    status = ifelse(e_late$eventtime == maxtime & c_middle$eventtime == maxtime, 0,\n                    ifelse(e_late$eventtime <= c_middle$eventtime, 1, 0))\n  )\n  late_late <- data.frame(\n    time = pmin(e_late$eventtime, c_late$eventtime),\n    status = ifelse(e_late$eventtime == maxtime & c_late$eventtime == maxtime, 0,\n                    ifelse(e_late$eventtime <= c_late$eventtime, 1, 0))\n  )\n  ee <- ee + 1/iter*(survfit( Surv( time, status ) ~ 1, data = early_early ) |>\n                       survfit_n() |>\n                       sf_to_df( time = time ))\n  ec <- ec + 1/iter*(survfit( Surv( time, status ) ~ 1, data = early_middle ) |>\n                       survfit_n() |>\n                       sf_to_df( time = time ))\n  el <- el + 1/iter*(survfit( Surv( time, status ) ~ 1, data = early_late ) |>\n                       survfit_n() |>\n                       sf_to_df( time = time ))\n  ce <- ce + 1/iter*(survfit( Surv( time, status ) ~ 1, data = middle_early ) |>\n                       survfit_n() |>\n                       sf_to_df( time = time ))\n  cc <- cc + 1/iter*(survfit( Surv( time, status ) ~ 1, data = middle_middle ) |>\n                       survfit_n() |>\n                       sf_to_df( time = time ))\n  cl <- cl + 1/iter*(survfit( Surv( time, status ) ~ 1, data = middle_late ) |>\n                       survfit_n() |>\n                       sf_to_df( time = time ))\n  le <- le + 1/iter*(survfit( Surv( time, status ) ~ 1, data = late_early ) |>\n                       survfit_n() |>\n                       sf_to_df( time = time ))\n  lc <- lc + 1/iter*(survfit( Surv( time, status ) ~ 1, data = late_middle ) |>\n                       survfit_n() |>\n                       sf_to_df( time = time ))\n  ll <- ll + 1/iter*(survfit( Surv( time, status ) ~ 1, data = late_late ) |>\n                       survfit_n() |>\n                       sf_to_df( time = time ))\n}\n\n\n\n\n# Early-early\nplot1 <- plot_effective_n( ee, mod = T, survival = F, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_x = F, ticks_y_sec = F, ylim = c(0, n) )\n\n# Early-middle\nplot2 <- plot_effective_n( ec, mod = T, survival = F, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_x = F, ticks_y = F, ticks_y_sec = F, ylim = c(0, n) )\n\n# Early-late\nplot3 <- plot_effective_n( el, mod = T, survival = F, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_x = F, ticks_y = F, ylim = c(0, n) )\n\n# Middle-early\nplot4 <- plot_effective_n( ce, mod = T, survival = F, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_x = F, ticks_y_sec = F, ylim = c(0, 1000) ) #+\n  #annotate(\"text\", x = 0, y = 480, label = \"Patients\", angle = 90)\n\n# Middle-middle\nplot5 <- plot_effective_n( cc, mod = T, survival = F, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_x = F, ticks_y = F, ticks_y_sec = F, ylim = c(0, n) )\n\n# Middle-late\nplot6 <- plot_effective_n( cl, mod = T, survival = F, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_x = F, ticks_y = F, ylim = c(0, n) ) #+\n  #annotate(\"text\", x = 5, y = 500, label = \"Incidence\", angle = 270)\n\n# Late-early\nplot7 <- plot_effective_n( le, mod = T, survival = F, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_y_sec = F, ylim = c(0, n) )\n\n# Late-middle\nplot8 <- plot_effective_n( lc, mod = T, survival = F, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_y = F, ticks_y_sec = F, ylim = c(0, n) ) #+\n  #annotate(\"text\", x = 2.5, y = 20, label = \"Time\")\n\n# Late-late\nplot9 <- plot_effective_n( ll, mod = T, survival = F, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_y = F, ylim = c(0, n) )\n\n# Combine plots into list\nplots <- list( plot1, plot2, plot3, plot4, plot5, plot6, plot7, plot8, plot9 )\n\n# Column and row labels\ncol_labels <- c(\"Early censoring\", \"Constant censoring\", \"Late censoring\")\nrow_labels <- c(\"Early events\", \"Constant events\", \"Late events\")\n\n# Create text grobs for column and row labels\ncol_text <- lapply(col_labels, textGrob, gp = gpar(fontsize = 14))\nrow_text <- lapply(row_labels, textGrob, gp = gpar(fontsize = 14), rot = 90)\n\n# Create a sample plot to extract the legend\nlegend_plot <- ggplot(data.frame(x = 1, y = 1, \n                                 color = c(\"Effective\", \"Modified\", \"Uncensored\", \"At risk\", \"Incidence\"), \n                                 linetype = c(\"Effective\", \"Modified\", \"Uncensored\", \"At risk\", \"Incidence\")), \n                      aes(x = x, y = y, color = color, linetype = linetype)) +\n  geom_line(linewidth = 1.3) +\n  scale_color_manual(values = c(\"Effective\"= \"#37293F\", \"Modified\" = \"#9f84af\", \"Uncensored\" = \"#c6aa2c\", \"At risk\" = \"#C2666B\"), name = \"Legend\") +\n  scale_linetype_manual(values = c(\"Effective\"= \"solid\", \"Modified\" = \"solid\", \"Uncensored\" = \"dotdash\", \"At risk\" = \"dashed\"), name = \"Legend\") +\n  theme(legend.position = \"right\") + theme_minimal()\n# \"Bounds\", \n# \"Bounds\" = \"#c6b5cf\", \n# \"Bounds\" = \"dashed\", \n\n\n# Extract the legend\nlegend <- cowplot::get_legend(legend_plot)\n\n# Create top row with column labels\ntop_row <- arrangeGrob(\n  grobs = c(list(textGrob(\"\")), col_text),\n  ncol = 4,\n  widths = c(0.1, 732/675, 1, 732/675)\n)\n\n# Create each row with plots and row label\nrows <- lapply(1:3, function(i) {\n  arrangeGrob(\n    grobs = c(list(row_text[[i]]), plots[((i-1)*3+1):(i*3)]),\n    ncol = 4,\n    widths = c(0.1, 732/675, 1, 732/675)\n  )\n})\n\n# Combine the top row and all plot rows into the final grid\nfinal_grid <- arrangeGrob(\n  grobs = c(list(top_row), rows),\n  ncol = 1,\n  heights = c(0.1, 1, 1, 827/800)\n)\n\n# Add legend to the grid\ncombined_grid <- arrangeGrob(\n  final_grid,\n  legend,\n  ncol = 2,\n  widths= c(8, 1)\n)\n\n\ngrid.arrange(combined_grid)\n\n\n#ggsave(file = \"images/sim2.svg\", plot=combined_grid, width = 12, height = 12, dpi = 700)\n#rm( shape, scale, e_early, e_middle, e_late, c_early, c_middle, c_late, \n#    early_early, early_middle, early_late, middle_early, middle_middle, \n#    middle_late, late_early, late_middle, late_late, plot1, plot2, plot3, plot4,\n#    plot5, plot6, plot7, plot8, plot9, plots, col_labels, row_labels, col_text, \n#    row_text, legend_plot, legend, top_row, rows, final_grid, combined_grid)\n```\n:::\n\n::: {.cell .hidden}\n\n```{.r .cell-code .hidden}\n# This is just to generate seperate KMs\n\n e_early <- simsurv( dist=\"weibull\", \n                         gammas=shape[1], \n                         lambdas=scale[1]^(-shape[1]), \n                         x=data.frame(id = 1:100000),\n                         maxt=maxtime)\n  p <- survfit( Surv( eventtime, status)~1, data = e_early)|>\n        survfit_n() |>\n        plot_km_eff2( xlab = \"Time (years)\")\n  ggsave(file = \"images/sim_km_e.svg\", plot=p, width = 7, height = 5)\n  \n  e_middle <- simsurv( dist=\"weibull\", \n                         gammas=shape[2], \n                         lambdas=scale[1]^(-shape[2]), \n                         x=data.frame(id = 1:100000),\n                         maxt=maxtime)\n    p <- survfit( Surv( eventtime, status)~1, data = e_middle)|>\n        survfit_n() |>\n        plot_km_eff2( xlab = \"Time (years)\")\n  ggsave(file = \"images/sim_km_m.svg\", plot=p, width = 7, height = 5)\n  \n  e_late <- simsurv( dist=\"weibull\", \n                         gammas=shape[3], \n                         lambdas=scale[1]^(-shape[3]), \n                         x=data.frame(id = 1:100000),\n                         maxt=maxtime)\n    p <- survfit( Surv( eventtime, status)~1, data = e_late)|>\n        survfit_n() |>\n        plot_km_eff2( xlab = \"Time (years)\")\n  ggsave(file = \"images/sim_km_l.svg\", plot=p, width = 7, height = 5)\n  \n  c_early <- simsurv( dist=\"weibull\", \n                         gammas=shape[1], \n                         lambdas=scale[2]^(-shape[1]), \n                         x=data.frame(id = 1:100000),\n                         maxt=maxtime)\n    p <- survfit( Surv( eventtime, status)~1, data = c_early)|>\n        survfit_n() |>\n        plot_km_eff2( xlab = \"Time (years)\")\n  ggsave(file = \"images/simc_km_e.svg\", plot=p, width = 7, height = 5)\n  \n  c_middle <- simsurv( dist=\"weibull\", \n                         gammas=shape[2], \n                         lambdas=scale[2]^(-shape[2]), \n                         x=data.frame(id = 1:100000),\n                         maxt=maxtime)\n    p <- survfit( Surv( eventtime, status)~1, data = c_middle)|>\n        survfit_n() |>\n        plot_km_eff2( xlab = \"Time (years)\")\n  ggsave(file = \"images/simc_km_m.svg\", plot=p, width = 7, height = 5)\n  \n  c_late <- simsurv( dist=\"weibull\", \n                         gammas=shape[3], \n                         lambdas=scale[2]^(-shape[3]), \n                         x=data.frame(id = 1:100000),\n                         maxt=maxtime)\n    p <- survfit( Surv( eventtime, status)~1, data = c_late)|>\n        survfit_n() |>\n        plot_km_eff2( xlab = \"Time (years)\")\n  #ggsave(file = \"images/simc_km_l.svg\", plot=p, width = 7, height = 5)\n```\n:::\n\n::: {#cell-fig-sim-same-risk-diff-ess .cell .hidden}\n\n```{.r .cell-code .hidden}\n# Seed for consistency\nset.seed(111923)\n\n# scale[1] for events, scale[2] for censoring 1 and scale[3] for censoring 2\nscale = c(2.5, 2.5, 2.5)\n# shape[1] for events, [2] for censoring 1 and [3] for censoring 2\nshape = c(1, 1/2, 2)\n# number of iterations\niter = 30\n# maxtime\nmaxtime = 5\n# timegrid for plot\ntime = seq(0, maxtime, 0.002)\n\n# So my setup is as follows. For the survival curve, I take the same for both, a Weibull( 1, 2.5 ), which is also an exponential distribution.\n# For the censoring, if I also use scale 2.5 for both of those, then the cumulative hazard for both the events and censoring will be 1 in both\n# simulations, which means that the number a risk must be the same as well (asymptotically, not every iteration of course). Then I can \n# change the effective sample size by varying the shape of the censoring distribution. If I change it too harshly, then the survival\n# function may be affected, so I choose 2 and 1/2 for it.\n\ndat1 <- dat2 <- 0\n# Multiple simulations - average 1 at a time\nfor ( i in 1:iter ){\n  # Generate the six datasets of 1000 patients\n  events <- simsurv( dist=\"weibull\", \n                         gammas=shape[1], \n                         lambdas=scale[1]^(-shape[1]), \n                         x=data.frame(id = 1:1000),\n                         maxt=maxtime)\n  censoring_1 <- simsurv( dist=\"weibull\", \n                         gammas=shape[2], \n                         lambdas=scale[2]^(-shape[2]), \n                         x=data.frame(id = 1:1000),\n                         maxt=maxtime)\n  censoring_2 <- simsurv( dist=\"weibull\", \n                         gammas=shape[3], \n                         lambdas=scale[3]^(-shape[3]), \n                         x=data.frame(id = 1:1000),\n                         maxt=maxtime)\n  \n  # Build the 2 dataframes for the figures\n  df_1 <- data.frame(\n    time = pmin(events$eventtime, censoring_1$eventtime),\n    status = ifelse(events$eventtime == maxtime & censoring_1$eventtime == maxtime, 0,\n                    ifelse(events$eventtime <= censoring_1$eventtime, 1, 0))\n  )\n  df_2 <- data.frame(\n    time = pmin(events$eventtime, censoring_2$eventtime),\n    status = ifelse(events$eventtime == maxtime & censoring_2$eventtime == maxtime, 0,\n                    ifelse(events$eventtime <= censoring_2$eventtime, 1, 0))\n  )\n  dat1 <- dat1 + 1/iter*(survfit( Surv( time, status ) ~ 1, data = df_1 ) |>\n                       survfit_n() |>\n                       sf_to_df( time = time ))\n  dat2 <- dat2 + 1/iter*(survfit( Surv( time, status ) ~ 1, data = df_2 ) |>\n                       survfit_n() |>\n                       sf_to_df( time = time ))\n}\n\nn <- 1000\ncolor_values <- c(\"Early Censoring\"= \"#2E7691\", \"Late Censoring\" = \"#C2666B\")\nlinetype_values <- c(\"Effective\"= \"solid\", \"At Risk\" = \"dashed\")\n# calculate the time where dat1$n.risk and dat2$n.risk cross over, that is, the bigger becomes smaller\nplot <- ggplot() +\n    geom_step(aes(x = dat1$time, y = dat1$n.eff, color = \"Early Censoring\", linetype = \"Effective\"), linewidth = 1.3) +\n    geom_step(aes(x = dat1$time, y = dat1$n.risk, color = \"Early Censoring\", linetype = \"At Risk\"), linewidth = 1.3) +\n    geom_step(aes(x = dat2$time, y = dat2$n.eff, color = \"Late Censoring\", linetype = \"Effective\"), linewidth = 1.3) +\n    geom_step(aes(x = dat2$time, y = dat2$n.risk, color = \"Late Censoring\", linetype = \"At Risk\"), linewidth = 1.3) +\n    labs(x = \"Time\", y = \"Number\") +\n    scale_x_continuous(breaks = seq(0, maxtime, length.out = 5)) +\n    theme_minimal() +\n    theme(legend.position.inside = c(0.1, 0.4), panel.grid.major.x = element_blank()) +\n    theme(text = element_text(size = 12), plot.title = element_text( size = 18, hjust = 0.3)) +\n    scale_color_manual(values = color_values, name = \"Legend\") +\n    scale_linetype_manual(values = linetype_values, name = \"Legend\") + \n    geom_vline(xintercept = 2.5, linetype = \"dotted\", lwd = 1.3)\nplot\nggsave(file = \"images/hein_sim_n.svg\", plot=plot, width = 7, height = 5)\n\nplot2 <- ggplot() +\n    geom_step(aes(x = dat1$time, y = dat1$surv, color = \"Early Censoring\"), linewidth = 1.3) +\n    geom_step(aes(x = dat2$time, y = dat2$surv, color = \"Late Censoring\"), linewidth = 1.3) +\n    labs(x = \"Time\", y = \"Survival probability\") +\n    scale_x_continuous(breaks = seq(0, maxtime, length.out = 5)) +\n    theme_minimal() +\n    theme(legend.position.inside = c(0.1, 0.4), panel.grid.major.x = element_blank()) +\n    theme(text = element_text(size = 12), plot.title = element_text( size = 18, hjust = 0.3)) +\n    scale_color_manual(values = color_values, name = \"Legend\") +\n    scale_linetype_manual(values = linetype_values, name = \"Legend\") + \n    geom_vline(xintercept = 2.5, linetype = \"dotted\", lwd = 1.3)\nplot2\n#ggsave(file = \"images/hein_sim_surv.svg\", plot=plot2, width = 7, height = 5)\n```\n:::\n\n\n\n\n\n\nComparisons between the rows in @fig-simulation illustrate how censoring hazards cause different patterns of $N_{\\text{eff, (mod)}}$ for the survival estimate. There are also differences in the numbers at risk between these scenarios at most times $t$. However, at a specific time point $t_*$, the number at risk may be exactly the same while there still are clear differences in $N_{\\text{eff, (mod)}}$ (Figure S3 in Supplement 4).\n\n## Effective sample size to guide study reporting\nEffective sample size can inform how much information is available for estimating the survival probability at the time point $t$ of interest. Maximum information is available when all patients have either experienced an event or reached the time point of interest event-free as uncensored observations.\nFor the colon data, we focus on the 5-year survival estimate $t=5$. Five years after start of the study, the first enrolled patient may have sufficient follow-up for a $t=5$ survival estimate. When accrual lasts 4 years, the last enrolled patient has 5 years of follow-up at year 9 after start of the study. We note that 95\\% of the effective sample size is already achieved at year 8 after start of the study, that is one year before the last patient reaches $t=5$ (@fig-trial). This pattern of available information was confirmed for other trial simulations (Figure S5 in Supplement 5).\n\n\n\n\n\n\n\n::: {#cell-fig-sim-trial .cell .hidden}\n\n```{.r .cell-code .hidden}\n# Seed for consistency\nset.seed(111923)\n\n# Scale and Shape\nscale = 3\nshape = 0.5\n\n# number of iterations\niter = 1\n# timegrid for plot\nmaxtime = 2\ntime = seq(0, maxtime, 0.002)\n\n\n# Multiple simulations - average 1 at a time\nfor ( i in 1:iter ){\n  # Generate the survival time\n  survival <- simsurv( dist=\"weibull\", \n                         gammas=shape, \n                         lambdas=scale^(-shape), \n                         x=data.frame(id = 1:250),\n                         maxt=maxtime)\n  \n  # Generate enrollment time\n  survival$tstart <- runif(250, 0, 2)\n  \n  # Survival in 'real' time since start study\n  survival$tstop <- survival$tstart + survival$eventtime\n  \n  \n  four <- data.frame(\n  time = survival$eventtime,\n  status = survival$status\n  )\n  \n  three <- data.frame(\n  time = ifelse( survival$tstop < 3, survival$eventtime, survival$eventtime - (survival$tstop - 3)),\n  status = ifelse( survival$tstop < 3, survival$status, 0)\n  )\n\n  two <- data.frame(\n  time = ifelse( survival$tstop < 2, survival$eventtime, survival$eventtime - (survival$tstop - 2)),\n  status = ifelse( survival$tstop < 2, survival$status, 0)\n  )\n}\n\n# Two years\nplot1 <- survfit( Surv(time, status) ~1, data = two ) |> \n  survfit_n() |>\n  plot_effective_n( mod = T, survival = T, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_x = T, ticks_y_sec = F )\n\n# Two half years\nplot2 <- survfit( Surv(time, status) ~1, data = three ) |> \n  survfit_n() |>\n  plot_effective_n( mod = T, survival = T, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_x = T, ticks_y_sec = F )\n\n# Three years\nplot3 <- survfit( Surv(time, status) ~1, data = four ) |> \n  survfit_n() |>\n  plot_effective_n( mod = T, survival = T, bounds = F, xlim = c(0, maxtime), labels = F, no_legend = T, ticks_x = T, ticks_y_sec = F )\n\n# Combine plots into list\nplots <- list( plot1, plot2, plot3 )\n\n# Column and row labels\ncol_labels <- c(\"Two years\", \"Three years\", \"Four years\")\n\n# Create text grobs for column and row labels\ncol_text <- lapply(col_labels, textGrob, gp = gpar(fontsize = 12))\n\n# Create a sample plot to extract the legend\nlegend_plot <- ggplot(data.frame(x = 1, y = 1, \n                                 color = c(\"Effective\", \"Modified\", \"Uncensored\", \"At risk\", \"Incidence\"), \n                                 linetype = c(\"Effective\", \"Modified\", \"Uncensored\", \"At risk\", \"Incidence\")), \n                      aes(x = x, y = y, color = color, linetype = linetype)) +\n  geom_line(linewidth = 1.3) +\n  scale_color_manual(values = c(\"Effective\"= \"#37293F\", \"At risk\" = \"#C2666B\", \"Uncensored\" = \"#2E7691\", \"Modified\" = \"#9f84af\", \"Incidence\" = \"#c6aa2c\"), name = \"Legend\") +\n  scale_linetype_manual(values = c(\"Effective\"= \"solid\", \"At risk\" = \"longdash\", \"Uncensored\" = \"dotdash\", \"Modified\" = \"solid\", \"Incidence\" = \"solid\"), name = \"Legend\") +\n  theme(legend.position = \"right\") + theme_minimal()\n# \"Bounds\", \n# \"Bounds\" = \"#c6b5cf\", \n# \"Bounds\" = \"dashed\", \n\n\n# Extract the legend\nlegend <- cowplot::get_legend(legend_plot)\n\n# Create top row with column labels\ntop_row <- arrangeGrob(\n  grobs = col_text,\n  ncol = 3,\n  widths = c(732/675, 1, 732/675)\n)\n\n# Create each row with plots and row label\nrows <- arrangeGrob(\n    grobs = plots[1:3],\n    ncol = 3,\n    widths = c(732/675, 1, 732/675)\n)\n\n# Combine the top row and all plot rows into the final grid\nfinal_grid <- arrangeGrob(\n  grobs = c(list(top_row), list(rows)),\n  ncol = 1,\n  heights = c(0.1, 1)\n)\n\n# Add legend to the grid\ncombined_grid <- arrangeGrob(\n  final_grid,\n  legend,\n  ncol = 2,\n  widths= c(7, 1)\n)\n\n# Display the final grid\ngrid.draw(combined_grid)\n```\n:::\n\n::: {#cell-fig-sim-reporting .cell}\n\n```{.r .cell-code .hidden}\n# Seed for consistency\nset.seed(111923)\n\nplot <- trial_report( gammas = c(0.5), \n              lambdas = c(3^(-0.5)), \n              x = data.frame( id = 1:500,\n                              treat = c(rep(0, 250), rep(1, 250))),\n              betas = c(treat = 0.7),\n              t_outcome = 2, \n              t_enroll = 2, \n              iter = 250, \n              n_patient = 250,\n              points = 25,\n              conf = T,\n              conf.int = 95,\n              proportion = F,\n              mod = F,\n              ylab = \"Effective Sample Size for 2 year survival\",\n              xlab = \"Time since study start\",\n              title = \"Effective sample size over reporting time\"\n              )\nplot\n#ggsave(file = \"images/trial_std.svg\", plot=plot, width = 7, height = 5)\n```\n:::\n\n\n\n\n\n\n![Illustration of modified effective sample size $N_{\\text{eff, mod}}$ of the 5-year overall survival Kaplan-Meier estimate at increasing trial reporting times in the colon data stratified by treatment arm. With accrual over 4 years and 5-year follow-up for the last patient, we note that 95% of the information in the study is already available at year 8, one year before the planned end of the study.](images/colon_trial.jpg){#fig-trial}\n\n\n# Discussion\nSample size is a challenging concept for time-to-event data, because not all patients may have complete follow-up (censoring). The concept of effective sample size can provide a measure with the same interpretation as sample size in standard statistical analyses of binary outcomes. We found that the pattern of the effective sample size depended on the hazard for censoring and the hazard for events. How rapidly effective sample size decreases depends especially on the hazard for censoring, with early censoring making survival estimates for later time points less reliable. We consider the modified version of effective sample size $N_{\\text{eff, mod}}$ attractive for a more natural behavior with Kaplan-Meier estimates with late censoring without late events, because $N_{\\text{eff, mod}}$ also decreases when there is only censoring. We discuss below why $N_{\\text{eff, (mod)}}$ might be valuable to add to Kaplan-Meier curves, for risk communication and for study reporting decisions.\n\n\n## Effective Sample Size below plots of Kaplan-Meier curves\nGuidelines and tutorials on the presentation of survival curves state that the number of patients at risk should be presented below the Kaplan-Meier curve  [@vickers2020; @morris2019; @jager2008; @pocock2002]. Often this is accompanied by a recommendation that the survival curve is truncated when the number at risk falls below a certain percentage of the number non-censored patients (10-20%) [@pocock2002], or below an absolute number (e.g. n<5, or n<10) [@vickers2020]. Number at risk is a measure of the reliability of the hazard at time $t$, which relates to the uncertainty for future survival probability estimates beyond time $t$. In contrast, $N_{\\text{eff, (mod)}}$ indicates the reliability of the estimated survival probability at time $t$, which is essential information for patients, physicians and other decision-makers. The contrast in absolute numbers can be large. For example. in the colon data the number at risk at 8 years of follow-up was only 26, while the $N_{\\text{eff}}$ of the estimated survival probability was 385 and $N_{\\text{eff, mod}}$ was 349.\nEffective sample size hence can be useful in addition to or instead of the number of patients at risk in a table below the curve. One might object that effective sample size provides information that is already included in the variance and reflected in a 95% confidence interval. Effective sample size however may be quite intuitive for communication of uncertainty with patients and other stakeholders. Further empirical studies are needed to assess this hypothesis, extending previous comparative research on presentations of Kaplan-Meier curves [@morris2019]. \n\n\n## Risk communication\nOpen communication about uncertainty is an important step in improving trustworthiness in scientific results [@spiegelhalter2017; @spiegelhalter2017address]. Some claimed that it is futile to try to explain uncertainty in predictions to individual patients [@kattan2011]. Also, domain experts, such as physicians, may be hesitant to communicate uncertainty, because such communication may undermine trust in their work [@fischhoff2012]. This is disputed by others [@vanderbles2020], while indeed people may be less likely to base decisions on numbers with low quality of evidence [@schneider2022].\nIn a well-cited study, the instruction with a nomogram for prostate cancer risk states that physicians should tell a patient: “if we had 100 men exactly like you, we would expect between $\\text{risk}\\%-10\\%$ and $\\text{risk}\\%+10\\%$ to remain free of their disease at 5 years” [@kattan2002]. Such a presentation may well convey the absolute risk to a patient but the presentation suggests that the prediction is based on 100 patients. This nomogram was based on Cox proportional hazards model. In predictions from logistic regression models, patients with exceptional covariate patterns have low effective sample [@thomassen2024]. Moreover, research into effective sample sizes of predictions from machine learning models warns that increased model flexibility reduces effective sample sizes of individual predictions [@thomassen2025].  Future research is needed to define and examine the effective sample size for survival predictions $\\hat S(t, \\mathbf{x}_{new})$ based on combinations of covariates.\nShared decision making between physicians and patients is increasingly important in modern medicine [@stiggelbout2015; @4Dpicture]. Communication about uncertainty of the evidence supporting treatment choices is important for the autonomy of patients [@parascandola2002]. Reference to uncertainty was noted in only half of consultations with probabilities, mentioned [@engelhardt2017]. Only 21% of those references to uncertainty were about the imprecision of the estimate. Effective sample size could help with communication of such imprecision, or epistemic uncertainty [@calin-jageman2019].\nAs an example we show a possible communication tool based on the colon data with effective sample size (Figure 5). For the key trial result, the definition of ‘patients like you’ refers to randomized patients who met the in- and exclusion criteria of the trial. For clinical application, in a real-world setting, important prognostic factors should be considered, and the format for presentation may need further refinement.\n\n\n\n![Example of using effective sample size in risk communication for the colon trial. The 8-year survival is estimated as 41% with surgery only and as 56% with the addition of Lev+5FU chemotherapy after surgery. The effective sample size sizes are 153 and 210 patients respectively.](images/communication.jpg){#fig-example}\n\nWe expect that uncertainty communication using effective sample size may not be preferred by all patients. Patients with high numeracy and statistical literacy may appreciate the 95% confidence interval for a survival prediction, while other patients may prefer simpler expressions of uncertainty such as verbal descriptions rather than numbers [@medendorp2021; @vromans2021].\n\n\n## Trial monitoring\nResearchers involved in prospective studies with a survival outcome may struggle with the timing of reporting of results. In oncological and other trials, it is common to require a minimum follow-up for the last patient. This causes a gap between inclusion of the last patient and knowledge on the results of a trial, while the results are important to guide further treatment policies. Reporting as early as possible may be pursued, therefore. As illustrated, effective sample size can express the relative information available at earlier versus later time points. For absolute values of sample size, based on a formal power calculation, a higher event rate than anticipated can positively impact the effective sample size. This may allow for earlier analysis and reporting after inclusion of the last patient. In contrast, more censoring by drop out than expected may argue for longer follow-up after the last included patient or increased enrollment for sufficient precision. \nWe recognize that changes in study design can inflate type I error rates [@proschan2009]. Further research is required to assess any possible impact on type I error rates from reporting decisions based on effective sample size.\n\n\n\n# Conclusion\nEffective sample size may be valuable to indicate the uncertainty of survival estimates from the Kaplan-Meier estimator, which has the same interpretation as sample size in studies without censoring. It can be shown below Kaplan-Meier plots and risk tables in addition to or instead of the number at risk, where it may yield an intuitive estimate of reliability for estimated survival probabilities until the time of interest. Future research should explore the role of effective sample size to communicate uncertainty, to guide reporting decisions, and to quantify uncertainty from regression or machine learning models with survival outcomes.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}